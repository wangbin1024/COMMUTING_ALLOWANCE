{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c77f267-7450-4c11-9b35-3b7f5d1295a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-sql-connector databricks-sdk langchain langchain-community databricks-langchain langchain_core langchain_community langgraph databricks-agents mlflow mlflow-skinny python-docx openpyxl pillow transformers torch uv langgraph==0.3.4 googlemaps pypdf unstructured databricks-vectorsearch python-docx openpyxl googlemaps google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3dcdadc6-23d4-4ea2-aad3-8aa81ed73bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers transformers==4.49.0\n",
    "%pip install fugashi ipadic unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae2cec9-2856-435a-9a11-6f7c70cab488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71e5d22d-5a96-4b61-ad2e-bf4cfa614826",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "データの保存場所を指定し、カタログとスキーマを作成"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from databricks.sdk.core import DatabricksError\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointStatusState, EndpointType\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointStateReady\n",
    "from databricks.sdk.errors import ResourceDoesNotExist, NotFound, PermissionDenied\n",
    "\n",
    "CURRENT_FOLDER = os.getcwd()\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# カタログとスキーマ\n",
    "UC_CATALOG = 'hhhd_demo_itec'\n",
    "UC_SCHEMA = 'commuting_allowance'\n",
    "\n",
    "# モデル\n",
    "UC_MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.commuting_allowance_model\"\n",
    "\n",
    "# search endpoint\n",
    "VECTOR_SEARCH_ENDPOINT = 'commuting_allowance_vector_search'\n",
    "\n",
    "# カタログを作成\n",
    "try:\n",
    "    _ = w.catalogs.get(UC_CATALOG)\n",
    "    print(f\"PASS: UC catalog `{UC_CATALOG}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.catalogs.create(name=UC_CATALOG)\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}` does not exist, and no permissions to create.  Please provide an existing UC Catalog.\")\n",
    "        raise ValueError(f\"Unity Catalog `{UC_CATALOG}` does not exist.\")\n",
    "        \n",
    "# スキーマを作成\n",
    "try:\n",
    "    _ = w.schemas.get(full_name=f\"{UC_CATALOG}.{UC_SCHEMA}\")\n",
    "    print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}.{UC_SCHEMA}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.schemas.create(name=UC_SCHEMA, catalog_name=UC_CATALOG)\n",
    "        print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` created\")\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}.{UC_SCHEMA}` does not exist, and no permissions to create.  Please provide an existing UC Schema.\")\n",
    "        raise ValueError(\"Unity Catalog Schema `{UC_CATALOG}.{UC_SCHEMA}` does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7842417e-99c6-4fe0-8d84-19a3f3df2bf1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ベクトル検索エンドポイントを作成"
    }
   },
   "outputs": [],
   "source": [
    "# エンドポイントが存在しない場合は作成する\n",
    "vector_search_endpoints = w.vector_search_endpoints.list_endpoints()\n",
    "if sum([VECTOR_SEARCH_ENDPOINT == ve.name for ve in vector_search_endpoints]) == 0:\n",
    "    print(f\"Please wait, creating Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}`.  This can take up to 10 minutes...\")\n",
    "    w.vector_search_endpoints.create_endpoint_and_wait(VECTOR_SEARCH_ENDPOINT, endpoint_type=EndpointType.STANDARD)\n",
    "\n",
    "# Make sure vector search endpoint is online and ready.\n",
    "w.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(VECTOR_SEARCH_ENDPOINT)\n",
    "\n",
    "print(f\"PASS: Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}` exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25080001-19aa-4dc4-800c-0b7b204a7836",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delta Table にデータを保存、インデックス作成 & 同期"
    }
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# SparkSessionを取得\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "workspace_url = SparkSession.getActiveSession().conf.get(\n",
    "    \"spark.databricks.workspaceUrl\", None\n",
    ")\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"\n",
    "    文書を適切なセクションごとに分割する。\n",
    "    - 空白行を境目として各セクションを分割\n",
    "    - 各「第X条 (タイトル)」や「附則」も個別のエントリとして処理\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # 空白行を境目にして分割\n",
    "    raw_sections = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "\n",
    "    for section in raw_sections:\n",
    "        # 「第X条」や「附則」がタイトルであるか判定\n",
    "        title_match = re.match(r'^(第\\s*\\d+\\s*条.*?)$', section, re.MULTILINE)\n",
    "\n",
    "        if title_match:\n",
    "            title = title_match.group(1).strip()\n",
    "            content = section[len(title):].strip()  # タイトル以外の本文\n",
    "            sections.append({\"title\": title, \"content\": content})\n",
    "        else:\n",
    "            # タイトルがないセクションもそのまま格納\n",
    "            sections.append({\"title\": \"\", \"content\": section.strip()})\n",
    "\n",
    "    return sections\n",
    "\n",
    "def chunk_text(text):\n",
    "    \"\"\"\n",
    "    条文ごとに適切にチャンクを作成する関数。\n",
    "    - 「第〇条」や「第〇項」を検出し、新しいチャンクを作成\n",
    "    - 各エントリを独立したチャンクとして保存\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "\n",
    "    # 空白行で分割\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    chunks = []\n",
    "    for sec in sections:\n",
    "        if sec['content']:\n",
    "            if sec['title']:\n",
    "                temp_chunk = f\"{sec['title']}:\\n{sec['content']}\"\n",
    "            else:\n",
    "                temp_chunk = sec['content']\n",
    "        else:\n",
    "            sec['title']\n",
    "        chunks.append(temp_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "pdf_path = \"通勤手当支給規程.pdf\"\n",
    "\n",
    "# PDFを読み込んでテキストに変換\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    reader = PdfReader(f)\n",
    "    text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "# PDFのテキストを適切な長さのチャンクに変換\n",
    "chunked_texts = chunk_text(text)\n",
    "\n",
    "# DataFrameに変換\n",
    "pdf_texts = [{\"chunk_id\": i, \"chunked_text\": chunk} for i, chunk in enumerate(chunked_texts)]\n",
    "\n",
    "# パスをVolumeに合わせて指定\n",
    "model_path = \"/Workspace/Users/wang-b2@itec.hankyu-hanshin.co.jp/ruri-base-v2\"\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# データをベクトル化\n",
    "embeddings = model.encode([item[\"chunked_text\"] for item in pdf_texts], show_progress_bar=True)\n",
    "\n",
    "# 各チャンクにembeddingを追加\n",
    "for i in range(len(pdf_texts)):\n",
    "  pdf_texts[i][\"embedding\"] = embeddings[i].tolist()\n",
    "\n",
    "# Deltaテーブルとして保存\n",
    "DELTA_TABLE_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.commuting_allowance_rules\"\n",
    "df = spark.createDataFrame(pdf_texts)\n",
    "\n",
    "# # double → float 配列への変換式\n",
    "df = df.withColumn(\"embedding\", expr(\"transform(embedding, x -> cast(x as float))\"))\n",
    "\n",
    "# 2. Deltaテーブルとして保存（上書き & schema更新）\n",
    "df.write.format(\"delta\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"overwriteSchema\", \"true\")\\\n",
    "  .saveAsTable(DELTA_TABLE_NAME)\n",
    "\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {DELTA_TABLE_NAME} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")\n",
    "\n",
    "# Databricks UI 上で Delta Table (databricks_docs_chunked) を確認できるリンクを表示\n",
    "# コードを実行したら、出力される URL をクリックし、Databricks UI に飛んで、Delta Table が正しく作成されたことを確認\n",
    "print(\n",
    "    f\"View Delta Table at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{DELTA_TABLE_NAME.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "# ベクトル検索インデックス\n",
    "CHUNKS_VECTOR_INDEX = f\"{UC_CATALOG}.{UC_SCHEMA}.commuting_allowance_index\"\n",
    "\n",
    "# ベクトル検索クライアントを取得\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# ベクトル検索インデックス（CHUNKS_VECTOR_INDEX）の作成を開始\n",
    "# コード実行後、URL をクリックして Databricks UI でインデックスが作成されているか確認\n",
    "print(\n",
    "    f\"Embedding docs & creating Vector Search Index, this will take ~5 - 10 minutes.\\nView Index Status at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_VECTOR_INDEX.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "# インデックスが存在している場合は作成しない\n",
    "try:\n",
    "    # インデックス作成 & 同期\n",
    "    index = vsc.create_delta_sync_index_and_wait(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "        source_table_name=DELTA_TABLE_NAME,\n",
    "        index_name=CHUNKS_VECTOR_INDEX,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"chunk_id\",\n",
    "        embedding_dimension=768,\n",
    "        embedding_vector_column=\"embedding\"\n",
    "    )\n",
    "    print(f\"Index {CHUNKS_VECTOR_INDEX} created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")\n",
    "    # print(f\"Index {CHUNKS_VECTOR_INDEX} already exists. Skipping index creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec0180f-0ac2-42db-9928-a5db44aa4ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME_1 = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "MODEL_NAME_2 = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "config = { \n",
    "    \"llm_model_serving_endpoint_name\": MODEL_NAME_1,\n",
    "}\n",
    "\n",
    "input_example = { \n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"AIエージェントとは？\",\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c31025-e695-4448-a409-c7f76f765537",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LangGraph　mlflow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "# TODO: Manually include underlying resources if needed. See the TODO in the markdown above for more information.\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=config[\"llm_model_serving_endpoint_name\"])]\n",
    "# for tool in tools:\n",
    "#     if isinstance(tool, VectorSearchRetrieverTool):\n",
    "#         resources.extend(tool.resources)\n",
    "#     elif isinstance(tool, UnityCatalogTool):\n",
    "#         resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example, \n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langgraph==0.3.4\",\n",
    "            \"databricks-langchain>=0.4.0\",\n",
    "            \"langchain-core\",\n",
    "            \"databricks-sql-connector\",\n",
    "            \"pypdf\",\n",
    "            \"python-docx\",\n",
    "            \"pandas\",\n",
    "            \"openpyxl\",\n",
    "            \"pyspark\",\n",
    "            \"googlemaps\",\n",
    "            \"requests\",\n",
    "            \"langchain\",\n",
    "            \"langchain-community\",\n",
    "            \"databricks-vectorsearch\",\n",
    "            \"sentence-transformers\",\n",
    "            \"transformers==4.49.0\",\n",
    "            \"google-cloud-vision\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153fa70c-778c-4d41-9d40-4dd4411e4fd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LangGraph　テスト用データ"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "content = \"'ユーザーID': 10099992, '保険区分': '住宅ローン', '保険内連番': null　こちらの年末調整情報を探して　住宅ローン控除.png　こちらの画像情報を探して、そしてを探した年末調整情報の結果と画像情報の結果を比較してください。\"\n",
    "\n",
    "# content = \"AIエージェントとは？\"\n",
    "# content = \"通勤手当申請書1.docx　こちらの通勤手当申請内容を取得して、会社規定に適合しているか判断してください。\"\n",
    "# content = \"通勤手当申請書1.docx　こちらの通勤手当申請書の内容を取得して、申請者が過去申請したことあるかどうかを検索して、そして今回の申請は会社規定に適合しているか判断してください。\"\n",
    "\n",
    "eval_examples = [\n",
    "    {\n",
    "        \"request\": {\"messages\": [{\"role\": \"user\", \"content\": content}]},\n",
    "        \"expected_response\": None,\n",
    "    },\n",
    "]\n",
    "eval_dataset = pd.DataFrame(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ef5880-3ff6-4fc5-925b-3b6884dfb874",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LangGraph　テスト"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_id=logged_agent_info.run_id):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "        data=eval_dataset,  # Your evaluation dataset\n",
    "        model_type=\"databricks-agent\",  # Enable Mosaic AI Agent Evaluation\n",
    "    )\n",
    "\n",
    "# Review the evaluation results in the MLFLow UI (see console output), or access them in place:\n",
    "# display(eval_results.tables['eval_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2eccb8f1-8cb2-4ee4-bf18-a57bb2dc64ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "デプロイ"
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import time\n",
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_CATALOG = 'hhhd_demo_itec'\n",
    "UC_SCHEMA = 'tax_adjustment'\n",
    "UC_MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.tax_adjustment_model\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"docs\"})\n",
    "\n",
    "# Wait for the Review App to be ready\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 9 - 10 minutes.\", end=\"\")\n",
    "while w.serving_endpoints.get(deployment_info.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment_info.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)\n",
    "\n",
    "print(f\"Endpoint {deployment_info.endpoint_name} is now ready!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
