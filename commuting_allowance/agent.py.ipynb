{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fe9969-ba89-426f-8e18-3596e6fc5e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq databricks-sql-connector databricks-sdk langchain langchain-community databricks-langchain langchain_core langchain_community langgraph databricks-agents mlflow mlflow-skinny python-docx openpyxl pillow transformers torch uv langgraph==0.3.4 google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b0291c-cf1b-4084-99c9-98e60890f57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "410fdb28-3d50-4cf2-9be5-fb91f8c8a8dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Optional, Sequence, Union, TypedDict, Literal, Annotated, Union, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import DatabricksVectorSearch, ChatDatabricks\n",
    "import mlflow\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentChunk, ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.tools import BaseTool, Tool\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langgraph.graph import END, StateGraph \n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from google.cloud import vision\n",
    "from databricks import sql\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "from datetime import date, datetime\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import operator\n",
    "import uuid\n",
    "\n",
    "############################################\n",
    "# LLM endpoint\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME_1 = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "LLM_ENDPOINT_NAME_2 = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "\n",
    "############################################\n",
    "# system prompt\n",
    "############################################\n",
    "system_prompt = \"\"\"\n",
    "ã‚ãªãŸã¯æ¥­å‹™æ”¯æ´ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n",
    "å¿…è¦ã«å¿œã˜ã¦é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚„æŒ‡ç¤ºã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# çŠ¶æ…‹ç®¡ç†ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã€€ChatAgentStateæ‹¡å¼µ\n",
    "class MyState(ChatAgentState):\n",
    "  tools_ran: Annotated[set[str], operator.or_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7387828-8375-4a21-a28a-cad4f0199c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# utilsé–¢æ•°\n",
    "def convert_dates(obj):\n",
    "  if isinstance(obj, (date, datetime)):\n",
    "    return obj.isoformat()\n",
    "  return obj\n",
    "\n",
    "def safe_json(obj):\n",
    "  if isinstance(obj, list):\n",
    "    return [safe_json(item) for item in obj]\n",
    "  elif isinstance(obj, dict):\n",
    "    return {k: safe_json(v) for k, v in obj.items()}\n",
    "  else:\n",
    "    return convert_dates(obj)\n",
    "\n",
    "def format_context(docs):\n",
    "  chunk_template = (\n",
    "    \"{chunk_text}\\n\\n\"\n",
    "  )\n",
    "  chunk_contents = [\n",
    "    chunk_template.format(\n",
    "      index=i + 1,\n",
    "      chunk_text=d.page_content.strip()\n",
    "    )\n",
    "    for i, d in enumerate(docs)\n",
    "  ]\n",
    "\n",
    "  contents = \"\".join(chunk_contents)\n",
    "  return contents\n",
    "\n",
    "def vector_search(state: MyState) -> str:\n",
    "    question = state[\"messages\"][-2][\"content\"]\n",
    "    print(f\"\\nğŸ“© vector_search\")\n",
    "    # Connect to the Vector Search Index\n",
    "    vs_client = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "    VECTOR_SEARCH_ENDPOINT = 'commuting_allowance_vector_search'\n",
    "    VECTOR_SEARCH_INDEX = 'hhhd_demo_itec.commuting_allowance.commuting_allowance_index'\n",
    "\n",
    "    # LangChain retrieverã®ä½œæˆ\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"/Workspace/Users/wang-b2@itec.hankyu-hanshin.co.jp/ruri-base-v2\"\n",
    "    )\n",
    "    vector_search_as_retriever = DatabricksVectorSearch(\n",
    "        endpoint=VECTOR_SEARCH_ENDPOINT,\n",
    "        index_name=VECTOR_SEARCH_INDEX,\n",
    "        embedding=embedding_model,\n",
    "        text_column=\"chunked_text\",\n",
    "        columns=[\"chunk_id\", \"chunked_text\"]\n",
    "    ).as_retriever(search_kwargs={\"k\": 8, \"score_threshold\": 0.7})\n",
    "\n",
    "    # result = vector_search_as_retriever.invoke(question)\n",
    "    result = vector_search_as_retriever.get_relevant_documents(question)\n",
    "    if len(result) > 0:\n",
    "        context = format_context(result)\n",
    "        tool_call = state[\"messages\"][-1][\"tool_calls\"][0]\n",
    "        arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        if \"__arg1\" in arguments:\n",
    "            arguments[\"__arg1\"] = context\n",
    "            \n",
    "        tool_call[\"function\"][\"arguments\"] = json.dumps(arguments, ensure_ascii=False)\n",
    "        return state\n",
    "    return state\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\"\n",
    "\n",
    "    # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ®µè½ï¼‰\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():  # ç©ºç™½è¡Œã‚’å‰Šé™¤\n",
    "            text += para.text.strip() + \"\\n\"\n",
    "\n",
    "    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n",
    "    for table in doc.tables:\n",
    "        processed_cells = set()  # æ—¢ã«å‡¦ç†ã—ãŸã‚»ãƒ«ã‚’ä¿å­˜\n",
    "\n",
    "        for row in table.rows:\n",
    "            row_data = []\n",
    "            for cell in row.cells:\n",
    "                cell_text = cell.text.strip()\n",
    "                if (cell._tc not in processed_cells) and cell_text:  # çµåˆã‚»ãƒ«ã‚’é™¤å¤–\n",
    "                    row_data.append(cell_text)\n",
    "                    processed_cells.add(cell._tc)\n",
    "                else:\n",
    "                    row_data.append(\"\")\n",
    "\n",
    "            text += \" | \".join(row_data) + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_text_from_xlsx(xlsx_path):\n",
    "    df = pd.read_excel(xlsx_path, engine='openpyxl', sheet_name=None) \n",
    "    text = \"\"\n",
    "    for sheet_name, sheet_df in df.items():\n",
    "        text += f\"### {sheet_name} ã‚·ãƒ¼ãƒˆ ###\\n\"\n",
    "        text += sheet_df.to_string(index=False, header=True) + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(txt_path):\n",
    "    with open(txt_path, \"r\", encoding=\"shift_jis\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def extract_text(file_path):\n",
    "    text = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if text == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif text == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif text in [\".xls\", \".xlsx\"]:\n",
    "        return extract_text_from_xlsx(file_path)\n",
    "    elif text == \".txt\":\n",
    "        return extract_text_from_txt(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™: {text}\")\n",
    "\n",
    "def create_llm_agent(model, tools, prompt):\n",
    "  agent = create_tool_calling_agent(model, tools, prompt)\n",
    "  return AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8781adf6-bac7-485a-a642-8bb4abc1b830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# tools\n",
    "############################################\n",
    "def get_tax_adjustment_info(datas):\n",
    "  try:\n",
    "    # JSONã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›\n",
    "    data = json.loads(datas)\n",
    "    user_id = data[\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID\"]\n",
    "    insurance_classification = data[\"ä¿é™ºåŒºåˆ†\"]\n",
    "    insurance_serial_number = data[\"ä¿é™ºå†…é€£ç•ª\"]\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM hhhd_demo_itec.tax_adjustment.`ä¿é™ºæ–™æ˜ç´°æƒ…å ±` \n",
    "    WHERE `ç”³è«‹çŠ¶æ³ï¼ˆä¿é™ºæ–™ï¼‰` <> 'å®Œäº†' AND `ãƒ¦ãƒ¼ã‚¶ãƒ¼ID` = '{user_id}' AND `ä¿é™ºåŒºåˆ†` = '{insurance_classification}'  \n",
    "    \"\"\"\n",
    "\n",
    "    if insurance_serial_number:\n",
    "        query += f\" AND `ä¿é™ºå†…é€£ç•ª` = '{insurance_serial_number}'\"\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    result = df.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "    print(f\"ç”³è«‹æƒ…å ±result: {result}\")\n",
    "\n",
    "    return json.dumps({\n",
    "      \"ç”³è«‹æƒ…å ±\": safe_json(result[0]) if result else {},\n",
    "      \"ä»¶æ•°\": len(result)\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "  except Exception as e:\n",
    "    return json.dumps({\n",
    "      \"ã‚¨ãƒ©ãƒ¼\": f\"ç”³è«‹æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "def ocr_tool(image_name):\n",
    "    try:\n",
    "        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’æŒ‡å®š\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Volumes/hhhd_demo_itec/tax_adjustment/keys/directed-reef-454701-d6-0f8e0d1de3c1.json\"\n",
    "\n",
    "        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        # OCRå¯¾è±¡ç”»åƒã®èª­ã¿è¾¼ã¿\n",
    "        image_path = f\"/Volumes/hhhd_demo_itec/tax_adjustment/images/{image_name}\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "            image = vision.Image(content=content)\n",
    "\n",
    "        # OCRå®Ÿè¡Œï¼ˆæ—¥æœ¬èªå«ã‚€ï¼‰\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f\"APIã‚¨ãƒ©ãƒ¼: {response.error.message}\")\n",
    "\n",
    "        if texts:\n",
    "            vision_text = texts[0].description\n",
    "            return json.dumps({\n",
    "                \"ç”»åƒæƒ…å ±\": {\n",
    "                \"ocrå…¨æ–‡\": vision_text.strip()\n",
    "                }\n",
    "            }, ensure_ascii=False)\n",
    "        else:\n",
    "            return json.dumps({\n",
    "                \"ç”»åƒæƒ…å ±\": {\n",
    "                \"ocrå…¨æ–‡\": \"\",\n",
    "                \"æ³¨æ„\": \"ãƒ†ã‚­ã‚¹ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "                }\n",
    "            }, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "        \"ã‚¨ãƒ©ãƒ¼\": f\"OCRå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "def check_tax_adjustment_consistency(datas):\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "    ã‚ãªãŸã¯å¹´æœ«èª¿æ•´ã®ç”³è«‹å†…å®¹ã¨ã€OCRã§èª­ã¿å–ã£ãŸä¿é™ºæ–™æ§é™¤è¨¼æ˜æ›¸ç”»åƒã®æƒ…å ±ã‚’ç…§åˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n",
    "    ç”Ÿå‘½ä¿é™ºã®å ´åˆã¯ä¿é™ºåˆ†é¡ãŒã‚ã‚Šã¾ã™ã€‚åŒã˜ä¿é™ºåˆ†é¡ã®æƒ…å ±ã‚’ç…§åˆã—ã¦ãã ã•ã„ã€‚\n",
    "    ä¿é™ºæ–™æ§é™¤è¨¼æ˜ã«ã¯11æœˆåˆ†ã¨12æœˆåˆ†ãŒåˆ†ã‹ã‚Œã¦è¨˜è¼‰ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã®å ´åˆã¯ç”³å‘Šé¡ã‚’åˆç®—ã—ã¦ãã ã•ã„ã€‚\n",
    "    ä¿é™ºæ–™æ§é™¤è¨¼æ˜æ›¸ã«è¤‡æ•°ç”³å‘Šå¹´ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€åŒã˜ç”³å‘Šå¹´ã®æƒ…å ±ã‚’ç…§åˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "    ã€ã‚¿ã‚¹ã‚¯ã€‘\n",
    "    ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦ã€ç”³è«‹æƒ…å ±ã¨ç”»åƒæƒ…å ±ã‚’æ¯”è¼ƒã—ã€ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚\n",
    "    - å¥‘ç´„è€…å\n",
    "    - ä¿é™ºä¼šç¤¾ãƒ»äº‹æ¥­æ‰€å\n",
    "    - ä¿é™ºåç§°\n",
    "    - ä¿é™ºæœŸé–“\n",
    "    - ç”Ÿå‘½ä¿é™ºåˆ†é¡ï¼ˆç”Ÿå‘½ä¿é™ºã®å ´åˆã®ã¿ï¼‰\n",
    "    - ä¿é™ºç¨®é¡\n",
    "    - ç”³å‘Šé¡\n",
    "    - ç”³å‘Šå¹´\n",
    "\n",
    "    ã€å‡ºåŠ›å½¢å¼ã€‘\n",
    "    ä»¥ä¸‹ã®ã‚ˆã†ãªèª­ã¿ã‚„ã™ã„å½¢å¼ã§ã€æ¯”è¼ƒçµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "    ---\n",
    "    ## å¥‘ç´„è€…åï¼š\n",
    "    - ç”³è«‹æƒ…å ±ï¼šXX å¤ªéƒ\n",
    "    - ç”»åƒæƒ…å ±ï¼šXX å¤ªéƒ\n",
    "    - ä¸€è‡´\n",
    "\n",
    "    ## ä¿é™ºåç§°ï¼š\n",
    "    - ç”³è«‹æƒ…å ±ï¼šXXä¿é™º\n",
    "    - ç”»åƒæƒ…å ±ï¼šOOä¿é™º\n",
    "    - ä¸ä¸€è‡´\n",
    "\n",
    "    ## ç·åˆè©•ä¾¡ï¼š\n",
    "    ---\n",
    "    \"\"\")\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "    ])\n",
    "    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "    extract_fields_chain = prompt | model | StrOutputParser()\n",
    "    result = extract_fields_chain.invoke({\"input\": datas})\n",
    "    return result\n",
    "\n",
    "def call_ehr_api_remand(data):\n",
    "    print(\"å·®æˆ»å‡¦ç†é–‹å§‹\")\n",
    "    # response = requests.get(\"https://api.hhhd.jp/ehr/remand\")\n",
    "\n",
    "    # if response.error.message:\n",
    "    #     raise Exception(f\"APIã‚¨ãƒ©ãƒ¼: {response.error.message}\")\n",
    "    # else:\n",
    "    #     return response.text\n",
    "    return \"å·®æˆ»ã—ã¾ã—ãŸ\"\n",
    "    \n",
    "def call_ehr_api_approval(data):\n",
    "    print(\"æ‰¿èªå‡¦ç†é–‹å§‹\")\n",
    "    # response = requests.get(\"https://api.hhhd.jp/ehr/approval\")\n",
    "\n",
    "    # if response.error.message:\n",
    "    #     raise Exception(f\"APIã‚¨ãƒ©ãƒ¼: {response.error.message}\")\n",
    "    # else:\n",
    "    #     return response.text\n",
    "    return \"æ‰¿èªã—ã¾ã—ãŸ\"\n",
    "\n",
    "def search_company_regulations(question: str) -> str:\n",
    "    return f\"ä¼šç¤¾è¦å®š:\\n\\n{question}\"\n",
    "\n",
    "def get_tax_adjustment_history(name: str) -> str:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    clean_name = name.replace(\" \", \"\")\n",
    "    if not clean_name:\n",
    "        print(\"åå‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "        return []\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT name, work_address, address, nearest_station, \n",
    "            route_1, distance_1, commuter_pass_1, \n",
    "            route_2, distance_2, commuter_pass_2\n",
    "    FROM hhhd_demo_itec.allowance_payment_rules.commuting_allowance_history\n",
    "    WHERE name = '{clean_name}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        text = json.dumps(df.collect(), ensure_ascii=False)\n",
    "        return f\"éå»ã®ç”³è«‹å±¥æ­´:\\n\\n{text}\"\n",
    "    except Exception as e:\n",
    "        print(f\"SQLå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        return \"ç”³è«‹å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    " \n",
    "def search_commute_routes(requests_data: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(requests_data)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\"})\n",
    "\n",
    "    api_key = \"test_nfY87YJYHMp\"\n",
    "    base_url = \"https://api.ekispert.jp/v1/json/search/course/extreme\"\n",
    "\n",
    "    from_address = data.get(\"from_address\")\n",
    "    to_address = data.get(\"to_address\")\n",
    "    \n",
    "    if not from_address or not to_address:\n",
    "        return json.dumps({\"error\": \"ã‚¨ãƒ©ãƒ¼: å‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã®ä½æ‰€ã¯å¿…é ˆã§ã™ã€‚\"})\n",
    "    print(f\"from_address: {from_address} to_address: {to_address}\")\n",
    "    radius = 1000\n",
    "    result_data = []\n",
    "\n",
    "    sort_types = {\n",
    "        \"time\": \"æœ€çŸ­ãƒ«ãƒ¼ãƒˆ\",\n",
    "        \"teiki\": \"æœ€å®‰ãƒ«ãƒ¼ãƒˆ\",\n",
    "        \"transfer\": \"ä¹—æ›å°‘ãªã„ãƒ«ãƒ¼ãƒˆ\"\n",
    "    }\n",
    "\n",
    "    for sort_key, label in sort_types.items():\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"viaList\": f\"{from_address},{radius}:{to_address},{radius}\",\n",
    "            \"searchType\": \"plain\",\n",
    "            \"sort\": sort_key,\n",
    "            \"answerCount\": 10,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                result_data.append({\"label\": label, \"error\": f\"APIã‚¨ãƒ©ãƒ¼: {response.status_code}\"})\n",
    "                continue\n",
    "\n",
    "            data = response.json()\n",
    "            course = data.get(\"ResultSet\", {}).get(\"Course\")\n",
    "            if isinstance(course, list):\n",
    "                course = course[0]\n",
    "\n",
    "            teiki = course.get(\"Teiki\", {})\n",
    "            displayRoute = teiki.get(\"DisplayRoute\", {})\n",
    "            route = course.get(\"Route\", {})\n",
    "            lines = route.get(\"Line\", [])\n",
    "            points = route.get(\"Point\", [])\n",
    "\n",
    "            timeOther = route.get(\"timeOther\", \"ä¸æ˜\")\n",
    "            timeOnBoard = route.get(\"timeOnBoard\", \"ä¸æ˜\")\n",
    "            timeWalk = route.get(\"timeWalk\", \"ä¸æ˜\")\n",
    "            distance = route.get(\"distance\", \"ä¸æ˜\")\n",
    "            transfer_count = route.get(\"transferCount\", \"ä¸æ˜\")\n",
    "\n",
    "            fare = \"ä¸æ˜\"\n",
    "            teiki1 = teiki3 = teiki6 = \"ä¸æ˜\"\n",
    "            for price in course.get(\"Price\", []):\n",
    "                kind = price.get(\"kind\") or price.get(\"Kind\")\n",
    "                if kind == \"FareSummary\":\n",
    "                    fare = price.get(\"Oneway\", \"ä¸æ˜\")\n",
    "                elif kind == \"Teiki1Summary\":\n",
    "                    teiki1 = price.get(\"Oneway\", \"ä¸æ˜\")\n",
    "                elif kind == \"Teiki3Summary\":\n",
    "                    teiki3 = price.get(\"Oneway\", \"ä¸æ˜\")\n",
    "                elif kind == \"Teiki6Summary\":\n",
    "                    teiki6 = price.get(\"Oneway\", \"ä¸æ˜\")\n",
    "\n",
    "            line_info = []\n",
    "            for i, line in enumerate(lines):\n",
    "                dep_idx = i\n",
    "                arr_idx = i + 1\n",
    "\n",
    "                dep_name = \"ï¼ˆå‡ºç™ºåœ°ä¸æ˜ï¼‰\"\n",
    "                arr_name = \"ï¼ˆåˆ°ç€åœ°ä¸æ˜ï¼‰\"\n",
    "\n",
    "                if dep_idx < len(points):\n",
    "                    point_dep = points[dep_idx]\n",
    "                    dep_name = (\n",
    "                        point_dep.get(\"Station\", {}).get(\"Name\")\n",
    "                        or point_dep.get(\"Name\")\n",
    "                        or dep_name\n",
    "                    )\n",
    "\n",
    "                if arr_idx < len(points):\n",
    "                    point_arr = points[arr_idx]\n",
    "                    arr_name = (\n",
    "                        point_arr.get(\"Station\", {}).get(\"Name\")\n",
    "                        or point_arr.get(\"Name\")\n",
    "                        or arr_name\n",
    "                    )\n",
    "\n",
    "                transport_name = line.get(\"Name\") or line.get(\"TypicalName\")\n",
    "\n",
    "                if not transport_name or transport_name.strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                line_info.append({\n",
    "                    \"transport_name\": transport_name,\n",
    "                    \"from\": dep_name,\n",
    "                    \"to\": arr_name\n",
    "                })\n",
    "\n",
    "            result_data.append({\n",
    "                \"label\": label,\n",
    "                \"time_on_board\": timeOnBoard,\n",
    "                \"time_other\": timeOther,\n",
    "                \"time_walk\": timeWalk,\n",
    "                \"total_time\": int(timeOnBoard) + int(timeWalk) + int(timeOther),\n",
    "                \"distance\": distance,\n",
    "                \"transfer_count\": transfer_count,\n",
    "                \"fare\": fare,\n",
    "                \"teiki_1\": teiki1,\n",
    "                \"teiki_3\": teiki3,\n",
    "                \"teiki_6\": teiki6,\n",
    "                \"route\": line_info\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            result_data.append({\"label\": label, \"error\": f\"ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {e}\"})\n",
    "\n",
    "    return json.dumps(result_data, ensure_ascii=False)\n",
    "\n",
    "def extract_text_from_file(file_name: str) -> str:\n",
    "    file_path = f\"/Volumes/hhhd_demo_itec/commuting_allowance/inputs/{file_name}\"\n",
    "\n",
    "    try:\n",
    "        extracted_text = extract_text(file_path)\n",
    "        text = f\"\\n---\\n\\n### {os.path.basename(file_path)} ã®ç”³è«‹å†…å®¹\\n\\n```\\n{extracted_text}\\n```\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {file_path} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        print(\"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ãŸã€ã‚‚ã—ãã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚\")\n",
    "        return \"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ãŸã€ã‚‚ã—ãã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚\"\n",
    "\n",
    "    try:\n",
    "        # questionã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã€å®šå‹æ–‡ã«å¤‰æ›ã™ã‚‹Chainã€€\n",
    "        extract_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "            ç”³è«‹æ›¸ãƒ‡ãƒ¼ã‚¿:\n",
    "            {input_text}                                                      \n",
    "\n",
    "            ä¸Šè¨˜ã®ç”³è«‹æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "            - ç”³è«‹è€…å\n",
    "            - å‹¤å‹™å…ˆä½æ‰€\n",
    "            - è‡ªå®…ä½æ‰€\n",
    "            - æœ€å¯„ã‚Šé§…\n",
    "            - åˆ©ç”¨äº¤é€šæ©Ÿé–¢ã¨çµŒè·¯\n",
    "            - é€šå‹¤è·é›¢\n",
    "            - å®šæœŸä»£\n",
    "\n",
    "            å®šæœŸä»£ãŒçµŒè·¯ã”ã¨ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã®å®šæœŸä»£ã‚’è¶³ã—ãŸç·é¡ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n",
    "            å‡ºåŠ›ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "            ---\n",
    "            ##ç”³è«‹æ›¸å†…å®¹##\n",
    "            - **ç”³è«‹è€…å**:  \n",
    "            - **å‹¤å‹™å…ˆä½æ‰€**:  \n",
    "            - **è‡ªå®…ä½æ‰€**:  \n",
    "            - **æœ€å¯„ã‚Šé§…**: \n",
    "            - **å®šæœŸä»£**: \n",
    "            - **åˆ©ç”¨äº¤é€šæ©Ÿé–¢ã¨çµŒè·¯â‘ **:  \n",
    "            - **é€šå‹¤è·é›¢â‘ **:   \n",
    "            ...ï¼ˆä»¥ä¸‹ç¶šãï¼‰\n",
    "            ---\n",
    "            \"\"\")\n",
    "        model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "        extract_fields_chain = extract_prompt_template | model | StrOutputParser()\n",
    "\n",
    "        llm_text = extract_fields_chain.invoke({\"input_text\": text})\n",
    "    except Exception as e:\n",
    "        print(f\"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        return text.strip()\n",
    "\n",
    "    return llm_text\n",
    "\n",
    "def check_commuting_allowance_consistency(datas):\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "    - ã‚ãªãŸã¯ä¼šç¤¾ã®é€šå‹¤æ‰‹å½“ã®ç”³è«‹ãŒé©æ­£ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚  \n",
    "    -ã€ç”³è«‹å†…å®¹ã€‘ã‚’ã€ä¼šç¤¾è¦å®šã€‘ã¨ç…§ã‚‰ã—åˆã‚ã›ã¦ã€ç”³è«‹ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚\n",
    "    -ã€éå»ã®ç”³è«‹å±¥æ­´ã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ãã®**ç”³è«‹å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨**ã€‚å±¥æ­´ãŒãªã„å ´åˆã¯ã€ç”³è«‹å±¥æ­´ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è¡¨ç¤ºã—ãªã„ã“ã¨ã€‚\n",
    "    -ã€æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ã‚ã‚ã›ã¦ã€åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚\n",
    "    - çµŒè·¯ãŒè¤‡æ•°ã«åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å®šæœŸä»£ã¯**åˆè¨ˆé‡‘é¡*ã§åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚ã¾ãŸã€é€šå‹¤è·é›¢ã¯**åˆè¨ˆè·é›¢**ã§åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚\n",
    "    - æœ€çµ‚åˆ¤æ–­ã¯ã€ä»¥ä¸‹ã®3ã¤ã‹ã‚‰é¸ã³ã€**å¿…ãšç†ç”±ã¨è¦å®šã®å¼•ç”¨ã‚’æ·»ãˆã¦å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚**\n",
    "    - **å•é¡Œãªã—**ï¼šã™ã¹ã¦ã®è¦å®šã«é©åˆã—ã¦ã„ã‚‹å ´åˆã€‚\n",
    "    - **å•é¡Œã‚ã‚Š**ï¼šä¸€ã¤ã§ã‚‚è¦å®šã«åã—ã¦ã„ã‚‹å ´åˆã€‚\n",
    "    - **è¦ç¢ºèª**ï¼šè¦å®šã«è©²å½“ãŒãªã„ã€åˆ¤æ–­ã§ããªã„ã€ã¾ãŸã¯åˆ¥é€”åŸºæº–ãŒå¿…è¦ãªå ´åˆã€‚\n",
    "    ---\n",
    "\n",
    "    ### **å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**\n",
    "\n",
    "    #### [ç”³è«‹è€…å] ã•ã‚“\n",
    "\n",
    "    - **éå»ã®ç”³è«‹å±¥æ­´**:  \n",
    "        - [ç”³è«‹å±¥æ­´å†…å®¹]  \n",
    "        - or `ãªã—`\n",
    "\n",
    "    - **æœ€å¯„ã‚Šé§…ç¢ºèªçµæœ**:  \n",
    "    - `ä¸€è‡´`  \n",
    "    - `ä¸ä¸€è‡´`ã€€æ¤œç´¢ã•ã‚ŒãŸæœ€å¯„ã‚Šé§…ï¼š[æ¤œç´¢ã•ã‚ŒãŸæœ€å¯„ã‚Šé§…å]\n",
    "\n",
    "    - **ä¼šç¤¾è¦å®šã®å¼•ç”¨ã¨åˆ¤æ–­ç†ç”±**:  \n",
    "        - `[å¼•ç”¨ã—ãŸä¼šç¤¾è¦å®š]`  \n",
    "        - `[è¦ç¨‹ã«åŸºã¥ã„ãŸè©³ç´°ãªåˆ¤æ–­ç†ç”±]`\n",
    "\n",
    "    - **æœ€çµ‚åˆ¤æ–­çµæœ**:  \n",
    "    - `å•é¡Œãªã—`  \n",
    "    - `å•é¡Œã‚ã‚Š`  \n",
    "    - `è¦ç¢ºèª`\n",
    "    \"\"\")\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "    ])\n",
    "    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "    extract_fields_chain = prompt | model | StrOutputParser()\n",
    "    result = extract_fields_chain.invoke({\"input\": datas})\n",
    "    return result\n",
    "\n",
    "def call_commuting_allowance_api_remand(data):\n",
    "    print(\"é€šå‹¤æ‰‹å½“ç”³è«‹å·®æˆ»å‡¦ç†é–‹å§‹\")\n",
    "    return \"é€šå‹¤æ‰‹å½“ç”³è«‹å·®æˆ»ã—ã¾ã—ãŸ\"\n",
    "    \n",
    "def call_commuting_allowance_api_approval(data):\n",
    "    print(\"é€šå‹¤æ‰‹å½“ç”³è«‹æ‰¿èªå‡¦ç†é–‹å§‹\")\n",
    "    return \"é€šå‹¤æ‰‹å½“ç”³è«‹æ‰¿èªã—ã¾ã—ãŸ\"\n",
    "\n",
    "# args_schemas\n",
    "class TaxAdjustmentArgs(BaseModel):\n",
    "  adjustment_info: dict = Field(\n",
    "    description=\"å¹´æœ«èª¿æ•´ç”³è«‹æƒ…å ±ã€‚å¥‘ç´„è€…åã€ä¿é™ºä¼šç¤¾åã€ä¿é™ºåç§°ã€å¹´é‡‘å¹´ã€ä¿é™ºè¨¼åˆ¸ç•ªå·ãªã©ã‚’å«ã‚€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã€‚\"\n",
    "  )\n",
    "  image_info: dict = Field(\n",
    "    description=\"OCRã«ã‚ˆã£ã¦æŠ½å‡ºã•ã‚ŒãŸä¿é™ºè¨¼æ˜æ›¸ç”»åƒã®å†…å®¹ã€‚å…¨æ–‡ã¾ãŸã¯é …ç›®åˆ¥ã®æƒ…å ±ã‚’å«ã‚€JSONå½¢å¼ã€‚\"\n",
    "  )\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"search_company_regulations\",\n",
    "        func=search_company_regulations,\n",
    "        description=\"\"\"\n",
    "        ä¼šç¤¾è¦å®šã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        ã€Œä¼šç¤¾è¦å®šã‚’å‚ç…§ã€ã¨è¨€ã‚ã‚ŒãŸå ´åˆã¯ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_tax_adjustment_history\",\n",
    "        func=get_tax_adjustment_history,\n",
    "        description=\"\"\"\n",
    "        é€šå‹¤æ‰‹å½“ã®éå»ã®ç”³è«‹å±¥æ­´ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        åå‰ã‚’ä½¿ã£ã¦ã€éå»ã®ç”³è«‹å±¥æ­´ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "        å…¥åŠ›ä¾‹: ã€Œç”³è«‹è€…ã®åå‰ã€\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_text_from_file\",\n",
    "        func=extract_text_from_file,\n",
    "        description=\"\"\"\n",
    "        ç”³è«‹æ›¸åã‚’æ¸¡ã—ã¦ã€ç”³è«‹æ›¸å†…å®¹ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        å…¥åŠ›ä¾‹: \"sample.pdf\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"search_commute_routes\",\n",
    "        func=search_commute_routes,\n",
    "        description= \"\"\"\n",
    "        JSONå½¢å¼ã®æ–‡å­—åˆ—ã§æ¸¡ã•ã‚ŒãŸå‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã®ä½æ‰€ã‚’ä½¿ã£ã¦ã€é§…ã™ã±ã‚ã¨APIã‹ã‚‰æœ€çŸ­ãƒ»æœ€é€Ÿãƒ»æœ€æ¥½ãƒ«ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        æ³¨æ„: å‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã¯å¿…ãšéƒ½é“åºœçœŒã‹ã‚‰å§‹ã¾ã‚‹å®Œå…¨ãªæ—¥æœ¬èªä½æ‰€ã«ã—ã¦ãã ã•ã„ã€‚ãã—ã¦ã€ãƒ“ãƒ«åãªã©ãŒè¦ã‚Šã¾ã›ã‚“ã€‚\n",
    "        å…¥åŠ›ä¾‹: '{\"from_address\": \"å¤§é˜ªåºœå¤§é˜ªå¸‚ä¸­å¤®åŒº1-1-1\", \"to_address\": \"å¤§é˜ªåºœå¤§é˜ªå¸‚ç¦å³¶åŒº1-2-3\"}'\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_commuting_allowance_api_remand\",\n",
    "        func=call_commuting_allowance_api_remand,\n",
    "        description=\"\"\"\n",
    "        é€šå‹¤æ‰‹å½“ç”³è«‹ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€é€šå‹¤æ‰‹å½“ç”³è«‹ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ã¨ã€å·®æˆ»çµæœãŒè¿”ã‚Šã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_commuting_allowance_api_approval\",\n",
    "        func=call_commuting_allowance_api_approval,\n",
    "        description=\"\"\"\n",
    "        é€šå‹¤æ‰‹å½“ç”³è«‹ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€é€šå‹¤æ‰‹å½“ç”³è«‹ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ã¨ã€æ‰¿èªçµæœãŒè¿”ã‚Šã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"check_commuting_allowance_consistency\",\n",
    "        func=check_commuting_allowance_consistency,\n",
    "        description=\"\"\"\n",
    "        é€šå‹¤æ‰‹å½“ç”³è«‹ãŒé©æ­£ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "\n",
    "        ä»¥ä¸‹4ã¤ã®å¼•æ•°ã‚’å¿…ãšæŒ‡å®šã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "        - ç”³è«‹å†…å®¹: é€šå‹¤æ‰‹å½“ç”³è«‹æ›¸ã®ç”³è«‹å†…å®¹\n",
    "        - ä¼šç¤¾è¦å®š: é€šå‹¤æ‰‹å½“ã«é–¢ã™ã‚‹ä¼šç¤¾è¦å®š\n",
    "        - éå»ã®ç”³è«‹å±¥æ­´: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ç”³è«‹ã—ãŸé€šå‹¤æ‰‹å½“ã®ç”³è«‹å±¥æ­´\n",
    "        - æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ: é§…ã™ã±ã‚ã¨APIã‹ã‚‰å–å¾—ã—ãŸæœ€çŸ­ãƒ»æœ€é€Ÿãƒ»æœ€æ¥½ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ\n",
    "\n",
    "        ä¾‹ï¼š\n",
    "        {\n",
    "        \"ç”³è«‹å†…å®¹\": { ... },\n",
    "        \"ä¼šç¤¾è¦å®š\": { ... },\n",
    "        \"éå»ã®ç”³è«‹å±¥æ­´\": { ... },\n",
    "        \"æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ\": { ... },\n",
    "        }\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_tax_adjustment_info\",\n",
    "        func=get_tax_adjustment_info,\n",
    "        description=\"\"\"\n",
    "        \"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID\", \"ä¿é™ºåŒºåˆ†\", \"ä¿é™ºå†…é€£ç•ª\"ã‚’ä½¿ã£ã¦ã€å¹´æœ«èª¿æ•´ã®ç”³è«‹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        å¿…ãšãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚ŒãŸ **æœ‰åŠ¹ãªJSONæ–‡å­—åˆ—** ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "        å…¥åŠ›ä¾‹: \"{\\\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID\\\": 0000001, \\\"ä¿é™ºåŒºåˆ†\\\": \\\"XXä¿é™º\\\", \\\"ä¿é™ºå†…é€£ç•ª\\\": 1}\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ocr_tool\",\n",
    "        func=ocr_tool,\n",
    "        description=\"\"\"\n",
    "        ç”»åƒåã‚’æ¸¡ã—ã¦ã€ç”»åƒæƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        å…¥åŠ›ä¾‹: \"image.jpg\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_ehr_api_remand\",\n",
    "        func=call_ehr_api_remand,\n",
    "        description=\"\"\"\n",
    "        e-ã˜ã‚“ã˜ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€e-ã˜ã‚“ã˜ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ã¨ã€å·®æˆ»çµæœãŒè¿”ã‚Šã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_ehr_api_approval\",\n",
    "        func=call_ehr_api_approval,\n",
    "        description=\"\"\"\n",
    "        e-ã˜ã‚“ã˜ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
    "        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€e-ã˜ã‚“ã˜ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ã¨ã€æ‰¿èªçµæœãŒè¿”ã‚Šã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"check_tax_adjustment_consistency\",\n",
    "        func=check_tax_adjustment_consistency,\n",
    "        args_schemas=TaxAdjustmentArgs,\n",
    "        description=\"\"\"\n",
    "        å¹´æœ«èª¿æ•´ç”³è«‹å†…å®¹ã¨ä¿é™ºè¨¼æ˜æ›¸ï¼ˆç”»åƒï¼‰ã®OCRçµæœã‚’æ¯”è¼ƒã—ã€ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚\n",
    "\n",
    "        ä»¥ä¸‹2ã¤ã®å¼•æ•°ã‚’å¿…ãšæŒ‡å®šã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "        - ç”³è«‹æƒ…å ±: å¹´æœ«èª¿æ•´ã®ç”³è«‹æƒ…å ±ï¼ˆè¾æ›¸å½¢å¼ï¼‰\n",
    "        - ç”»åƒæƒ…å ±: OCRã«ã‚ˆã£ã¦å–å¾—ã—ãŸç”»åƒæƒ…å ±ï¼ˆè¾æ›¸å½¢å¼ï¼‰\n",
    "\n",
    "        ä¾‹ï¼š\n",
    "        {\n",
    "        \"ç”³è«‹æƒ…å ±\": { ... },\n",
    "        \"ç”»åƒæƒ…å ±\": { ... }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff9dd9c-3dc6-4f0b-9e5d-12f6126de9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "# Human-in-the-loop\n",
    "############################################\n",
    "# äººé–“å‚ä¸\n",
    "def human_assistance(state: MyState):\n",
    "    print(\"### ç®¡ç†è€…ã®ç¢ºèªå¾…ã¡ ###\")\n",
    "    # \"name\": \"check_tax_adjustment_consistency\n",
    "    last_query = state[\"messages\"][-2][\"content\"]\n",
    "    print(f\"\\nğŸ“© ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆ¤æ–­çµæœ:\\n{last_query}\")\n",
    "\n",
    "    response_text = input(\"æ¬¡ã®ã‚„ã‚‹ã¹ãã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \")\n",
    "\n",
    "    # tools_ran ã‚’æ›´æ–°ï¼ˆè©²å½“ãƒ„ãƒ¼ãƒ«ã‚’å‰Šé™¤ï¼‰\n",
    "    updated_tools_ran = set(state.get(\"tools_ran\", set()))\n",
    "    updated_tools_ran.discard(\"check_tax_adjustment_consistency\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": response_text,\n",
    "        \"id\": str(uuid.uuid4())\n",
    "        }],\n",
    "        \"tools_ran\": updated_tools_ran\n",
    "    }\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å‘¼ã³å‡ºã—ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "def custom_tool_node(state: dict) -> dict:\n",
    "    # ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆLangGraphå†…éƒ¨ã‹ã‚‰ã®å‘¼ã³å‡ºã—ï¼‰\n",
    "    tool_node = ChatAgentToolNode(tools)\n",
    "    tool_result_state = tool_node.invoke(state, config=None)\n",
    "\n",
    "    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾—ï¼ˆAIMessage or dictï¼‰\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "\n",
    "    # tool_calls å–å¾—ï¼ˆä¸¡å¯¾å¿œï¼šdict or AIMessageï¼‰\n",
    "    if isinstance(last_msg, dict):\n",
    "        tool_calls = last_msg.get(\"tool_calls\", [])\n",
    "    else:\n",
    "        tool_calls = getattr(last_msg, \"tool_calls\", [])\n",
    "\n",
    "    # ã™ã§ã«å®Ÿè¡Œæ¸ˆã¿ã® tool ã®ã‚»ãƒƒãƒˆ\n",
    "    tools_ran = set(state.get(\"tools_ran\", set()))\n",
    "\n",
    "    # å®Ÿè¡Œã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«åã‚’ tools_ran ã«è¿½åŠ \n",
    "    for call in tool_calls:\n",
    "        # call è‡ªä½“ã‚‚ dict ã¾ãŸã¯ pydantic model ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ä¸¡å¯¾å¿œ\n",
    "        if isinstance(call, dict):\n",
    "            tool_name = call.get(\"name\")\n",
    "        else:\n",
    "            tool_name = getattr(call, \"name\", None)\n",
    "        if tool_name:\n",
    "            tools_ran.add(tool_name)\n",
    "\n",
    "    # æ›´æ–°å¾Œã®çŠ¶æ…‹ã« tools_ran ã‚’è¿½åŠ \n",
    "    tool_result_state[\"tools_ran\"] = tools_ran\n",
    "    return tool_result_state\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "# LangGraphãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    # æ¬¡ã®çŠ¶æ…‹ã‚’åˆ¤æ–­ã™ã‚‹é–¢æ•°\n",
    "    def should_continue(state: MyState) -> str:\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        tools_ran = state.get(\"tools_ran\", set())\n",
    "        if \"tool_calls\" in last_msg and last_msg[\"tool_calls\"]:\n",
    "            function_name = last_msg[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "            if function_name == \"search_company_regulations\":\n",
    "                return \"vector_search\"\n",
    "            return \"tools\"\n",
    "        elif \"check_tax_adjustment_consistency\" in tools_ran:\n",
    "            tools_ran.discard(\"check_tax_adjustment_consistency\")\n",
    "            return \"human\"\n",
    "        elif \"check_commuting_allowance_consistency\" in tools_ran:\n",
    "            tools_ran.discard(\"check_commuting_allowance_consistency\")\n",
    "            return \"human\"\n",
    "        return \"end\"\n",
    "\n",
    "    # å‰å‡¦ç†ï¼ˆå…¥åŠ›ã•ã‚ŒãŸ MyState ã‹ã‚‰ \"messages\" ã‚’å–ã‚Šå‡ºã—ã€å¿…è¦ãªã‚‰ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«è¿½åŠ ï¼‰\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©\n",
    "    model_runnable = preprocessor | model\n",
    "    \n",
    "    def call_model(state: MyState, config: RunnableConfig):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        if response is None:\n",
    "            return {\"messages\": []}\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # æ–°ã—ã„çŠ¶æ…‹stateã‚’å®šç¾©\n",
    "    workflow = StateGraph(MyState)\n",
    "    # nodeã‚’è¿½åŠ \n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", RunnableLambda(custom_tool_node))\n",
    "    workflow.add_node(\"human\", RunnableLambda(human_assistance))\n",
    "    workflow.add_node(\"vector_search\", RunnableLambda(vector_search))\n",
    "\n",
    "    ###########################\n",
    "    ## çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©\n",
    "    ## agent â†’ \"tool_calls\"ã‚ã‚Š â†’ tools\n",
    "    ## tools â†’ å‘¼ã³å‡ºã—å¾Œ â†’ agent ã«æˆ»ã‚‹ï¼ˆå†å¿œç­”ï¼‰\n",
    "    ###########################\n",
    "    # å…¥å£\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    # æ¡ä»¶edgeè¿½åŠ \n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"human\": \"human\",\n",
    "            \"vector_search\": \"vector_search\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # ä¸€èˆ¬edgeè¿½åŠ \n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    workflow.add_edge(\"human\", \"agent\")\n",
    "    workflow.add_edge(\"vector_search\", \"tools\")\n",
    "    # ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ \n",
    "    memory = MemorySaver()\n",
    "    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "# å‡ºåŠ›çµæœã®å½¢ã‚’æ•´å½¢ã™ã‚‹\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        config = {\"configurable\": {\"thread_id\": context.thread_id if context else \"session_001\"}}\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, config=config, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        config = {\"configurable\": {\"thread_id\": context.thread_id if context else \"session_001\"}}\n",
    "        for event in self.agent.stream(request, config=config, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)\n",
    "\n",
    "# æµã‚Œå›³ç”Ÿæˆ\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     print(\"Failed to render graph.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agent.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
