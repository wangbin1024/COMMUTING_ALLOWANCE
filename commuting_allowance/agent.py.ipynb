{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fe9969-ba89-426f-8e18-3596e6fc5e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq databricks-sql-connector databricks-sdk langchain langchain-community databricks-langchain langchain_core langchain_community langgraph databricks-agents mlflow mlflow-skinny python-docx openpyxl pillow transformers torch uv langgraph==0.3.4 google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b0291c-cf1b-4084-99c9-98e60890f57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "410fdb28-3d50-4cf2-9be5-fb91f8c8a8dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Optional, Sequence, Union, TypedDict, Literal, Annotated, Union, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import DatabricksVectorSearch, ChatDatabricks\n",
    "import mlflow\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentChunk, ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.tools import BaseTool, Tool\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langgraph.graph import END, StateGraph \n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from google.cloud import vision\n",
    "from databricks import sql\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "from datetime import date, datetime\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import operator\n",
    "import uuid\n",
    "\n",
    "############################################\n",
    "# LLM endpoint\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME_1 = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "LLM_ENDPOINT_NAME_2 = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "\n",
    "############################################\n",
    "# system prompt\n",
    "############################################\n",
    "system_prompt = \"\"\"\n",
    "あなたは業務支援のAIアシスタントです。\n",
    "必要に応じて適切なツールを使って、ユーザーの質問や指示に対応してください。\n",
    "\"\"\"\n",
    "\n",
    "# 状態管理カスタマイズ　ChatAgentState拡張\n",
    "class MyState(ChatAgentState):\n",
    "  tools_ran: Annotated[set[str], operator.or_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7387828-8375-4a21-a28a-cad4f0199c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# utils関数\n",
    "def convert_dates(obj):\n",
    "  if isinstance(obj, (date, datetime)):\n",
    "    return obj.isoformat()\n",
    "  return obj\n",
    "\n",
    "def safe_json(obj):\n",
    "  if isinstance(obj, list):\n",
    "    return [safe_json(item) for item in obj]\n",
    "  elif isinstance(obj, dict):\n",
    "    return {k: safe_json(v) for k, v in obj.items()}\n",
    "  else:\n",
    "    return convert_dates(obj)\n",
    "\n",
    "def format_context(docs):\n",
    "  chunk_template = (\n",
    "    \"{chunk_text}\\n\\n\"\n",
    "  )\n",
    "  chunk_contents = [\n",
    "    chunk_template.format(\n",
    "      index=i + 1,\n",
    "      chunk_text=d.page_content.strip()\n",
    "    )\n",
    "    for i, d in enumerate(docs)\n",
    "  ]\n",
    "\n",
    "  contents = \"\".join(chunk_contents)\n",
    "  return contents\n",
    "\n",
    "def vector_search(state: MyState) -> str:\n",
    "    question = state[\"messages\"][-2][\"content\"]\n",
    "    print(f\"\\n📩 vector_search\")\n",
    "    # Connect to the Vector Search Index\n",
    "    vs_client = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "    VECTOR_SEARCH_ENDPOINT = 'commuting_allowance_vector_search'\n",
    "    VECTOR_SEARCH_INDEX = 'hhhd_demo_itec.commuting_allowance.commuting_allowance_index'\n",
    "\n",
    "    # LangChain retrieverの作成\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"/Workspace/Users/wang-b2@itec.hankyu-hanshin.co.jp/ruri-base-v2\"\n",
    "    )\n",
    "    vector_search_as_retriever = DatabricksVectorSearch(\n",
    "        endpoint=VECTOR_SEARCH_ENDPOINT,\n",
    "        index_name=VECTOR_SEARCH_INDEX,\n",
    "        embedding=embedding_model,\n",
    "        text_column=\"chunked_text\",\n",
    "        columns=[\"chunk_id\", \"chunked_text\"]\n",
    "    ).as_retriever(search_kwargs={\"k\": 8, \"score_threshold\": 0.7})\n",
    "\n",
    "    # result = vector_search_as_retriever.invoke(question)\n",
    "    result = vector_search_as_retriever.get_relevant_documents(question)\n",
    "    if len(result) > 0:\n",
    "        context = format_context(result)\n",
    "        tool_call = state[\"messages\"][-1][\"tool_calls\"][0]\n",
    "        arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        if \"__arg1\" in arguments:\n",
    "            arguments[\"__arg1\"] = context\n",
    "            \n",
    "        tool_call[\"function\"][\"arguments\"] = json.dumps(arguments, ensure_ascii=False)\n",
    "        return state\n",
    "    return state\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\"\n",
    "\n",
    "    # 通常のテキスト（段落）\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():  # 空白行を削除\n",
    "            text += para.text.strip() + \"\\n\"\n",
    "\n",
    "    # テーブルデータの取得\n",
    "    for table in doc.tables:\n",
    "        processed_cells = set()  # 既に処理したセルを保存\n",
    "\n",
    "        for row in table.rows:\n",
    "            row_data = []\n",
    "            for cell in row.cells:\n",
    "                cell_text = cell.text.strip()\n",
    "                if (cell._tc not in processed_cells) and cell_text:  # 結合セルを除外\n",
    "                    row_data.append(cell_text)\n",
    "                    processed_cells.add(cell._tc)\n",
    "                else:\n",
    "                    row_data.append(\"\")\n",
    "\n",
    "            text += \" | \".join(row_data) + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_text_from_xlsx(xlsx_path):\n",
    "    df = pd.read_excel(xlsx_path, engine='openpyxl', sheet_name=None) \n",
    "    text = \"\"\n",
    "    for sheet_name, sheet_df in df.items():\n",
    "        text += f\"### {sheet_name} シート ###\\n\"\n",
    "        text += sheet_df.to_string(index=False, header=True) + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(txt_path):\n",
    "    with open(txt_path, \"r\", encoding=\"shift_jis\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def extract_text(file_path):\n",
    "    text = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if text == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif text == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif text in [\".xls\", \".xlsx\"]:\n",
    "        return extract_text_from_xlsx(file_path)\n",
    "    elif text == \".txt\":\n",
    "        return extract_text_from_txt(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"対応していないファイル形式です: {text}\")\n",
    "\n",
    "def create_llm_agent(model, tools, prompt):\n",
    "  agent = create_tool_calling_agent(model, tools, prompt)\n",
    "  return AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8781adf6-bac7-485a-a642-8bb4abc1b830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# tools\n",
    "############################################\n",
    "def get_tax_adjustment_info(datas):\n",
    "  try:\n",
    "    # JSONからオブジェクトに変換\n",
    "    data = json.loads(datas)\n",
    "    user_id = data[\"ユーザーID\"]\n",
    "    insurance_classification = data[\"保険区分\"]\n",
    "    insurance_serial_number = data[\"保険内連番\"]\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM hhhd_demo_itec.tax_adjustment.`保険料明細情報` \n",
    "    WHERE `申請状況（保険料）` <> '完了' AND `ユーザーID` = '{user_id}' AND `保険区分` = '{insurance_classification}'  \n",
    "    \"\"\"\n",
    "\n",
    "    if insurance_serial_number:\n",
    "        query += f\" AND `保険内連番` = '{insurance_serial_number}'\"\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    result = df.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "    print(f\"申請情報result: {result}\")\n",
    "\n",
    "    return json.dumps({\n",
    "      \"申請情報\": safe_json(result[0]) if result else {},\n",
    "      \"件数\": len(result)\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "  except Exception as e:\n",
    "    return json.dumps({\n",
    "      \"エラー\": f\"申請情報取得中にエラーが発生しました: {str(e)}\"\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "def ocr_tool(image_name):\n",
    "    try:\n",
    "        # サービスアカウントキーを指定\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Volumes/hhhd_demo_itec/tax_adjustment/keys/directed-reef-454701-d6-0f8e0d1de3c1.json\"\n",
    "\n",
    "        # Vision APIクライアントの作成\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        # OCR対象画像の読み込み\n",
    "        image_path = f\"/Volumes/hhhd_demo_itec/tax_adjustment/images/{image_name}\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "            image = vision.Image(content=content)\n",
    "\n",
    "        # OCR実行（日本語含む）\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f\"APIエラー: {response.error.message}\")\n",
    "\n",
    "        if texts:\n",
    "            vision_text = texts[0].description\n",
    "            return json.dumps({\n",
    "                \"画像情報\": {\n",
    "                \"ocr全文\": vision_text.strip()\n",
    "                }\n",
    "            }, ensure_ascii=False)\n",
    "        else:\n",
    "            return json.dumps({\n",
    "                \"画像情報\": {\n",
    "                \"ocr全文\": \"\",\n",
    "                \"注意\": \"テキストが検出されませんでした\"\n",
    "                }\n",
    "            }, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "        \"エラー\": f\"OCR処理中にエラーが発生しました: {str(e)}\"\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "def check_tax_adjustment_consistency(datas):\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "    あなたは年末調整の申請内容と、OCRで読み取った保険料控除証明書画像の情報を照合するAIアシスタントです。\n",
    "    生命保険の場合は保険分類があります。同じ保険分類の情報を照合してください。\n",
    "    保険料控除証明には11月分と12月分が分かれて記載される場合があります。この場合は申告額を合算してください。\n",
    "    保険料控除証明書に複数申告年が書かれている場合は、同じ申告年の情報を照合してください。\n",
    "\n",
    "    【タスク】\n",
    "    以下の項目について、申請情報と画像情報を比較し、一致しているかどうかを判断してください。\n",
    "    - 契約者名\n",
    "    - 保険会社・事業所名\n",
    "    - 保険名称\n",
    "    - 保険期間\n",
    "    - 生命保険分類（生命保険の場合のみ）\n",
    "    - 保険種類\n",
    "    - 申告額\n",
    "    - 申告年\n",
    "\n",
    "    【出力形式】\n",
    "    以下のような読みやすい形式で、比較結果を記述してください：\n",
    "\n",
    "    ---\n",
    "    ## 契約者名：\n",
    "    - 申請情報：XX 太郎\n",
    "    - 画像情報：XX 太郎\n",
    "    - 一致\n",
    "\n",
    "    ## 保険名称：\n",
    "    - 申請情報：XX保険\n",
    "    - 画像情報：OO保険\n",
    "    - 不一致\n",
    "\n",
    "    ## 総合評価：\n",
    "    ---\n",
    "    \"\"\")\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "    ])\n",
    "    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "    extract_fields_chain = prompt | model | StrOutputParser()\n",
    "    result = extract_fields_chain.invoke({\"input\": datas})\n",
    "    return result\n",
    "\n",
    "def call_ehr_api_remand(data):\n",
    "    print(\"差戻処理開始\")\n",
    "    # response = requests.get(\"https://api.hhhd.jp/ehr/remand\")\n",
    "\n",
    "    # if response.error.message:\n",
    "    #     raise Exception(f\"APIエラー: {response.error.message}\")\n",
    "    # else:\n",
    "    #     return response.text\n",
    "    return \"差戻しました\"\n",
    "    \n",
    "def call_ehr_api_approval(data):\n",
    "    print(\"承認処理開始\")\n",
    "    # response = requests.get(\"https://api.hhhd.jp/ehr/approval\")\n",
    "\n",
    "    # if response.error.message:\n",
    "    #     raise Exception(f\"APIエラー: {response.error.message}\")\n",
    "    # else:\n",
    "    #     return response.text\n",
    "    return \"承認しました\"\n",
    "\n",
    "def search_company_regulations(question: str) -> str:\n",
    "    return f\"会社規定:\\n\\n{question}\"\n",
    "\n",
    "def get_tax_adjustment_history(name: str) -> str:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    clean_name = name.replace(\" \", \"\")\n",
    "    if not clean_name:\n",
    "        print(\"名前が指定されていないため、検索をスキップします。\")\n",
    "        return []\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT name, work_address, address, nearest_station, \n",
    "            route_1, distance_1, commuter_pass_1, \n",
    "            route_2, distance_2, commuter_pass_2\n",
    "    FROM hhhd_demo_itec.allowance_payment_rules.commuting_allowance_history\n",
    "    WHERE name = '{clean_name}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        text = json.dumps(df.collect(), ensure_ascii=False)\n",
    "        return f\"過去の申請履歴:\\n\\n{text}\"\n",
    "    except Exception as e:\n",
    "        print(f\"SQL実行中にエラーが発生しました: {e}\")\n",
    "        return \"申請履歴が見つかりませんでした。\"\n",
    " \n",
    "def search_commute_routes(requests_data: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(requests_data)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"JSONのパースに失敗しました: {e}\"})\n",
    "\n",
    "    api_key = \"test_nfY87YJYHMp\"\n",
    "    base_url = \"https://api.ekispert.jp/v1/json/search/course/extreme\"\n",
    "\n",
    "    from_address = data.get(\"from_address\")\n",
    "    to_address = data.get(\"to_address\")\n",
    "    \n",
    "    if not from_address or not to_address:\n",
    "        return json.dumps({\"error\": \"エラー: 出発地と到着地の住所は必須です。\"})\n",
    "    print(f\"from_address: {from_address} to_address: {to_address}\")\n",
    "    radius = 1000\n",
    "    result_data = []\n",
    "\n",
    "    sort_types = {\n",
    "        \"time\": \"最短ルート\",\n",
    "        \"teiki\": \"最安ルート\",\n",
    "        \"transfer\": \"乗換少ないルート\"\n",
    "    }\n",
    "\n",
    "    for sort_key, label in sort_types.items():\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"viaList\": f\"{from_address},{radius}:{to_address},{radius}\",\n",
    "            \"searchType\": \"plain\",\n",
    "            \"sort\": sort_key,\n",
    "            \"answerCount\": 10,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                result_data.append({\"label\": label, \"error\": f\"APIエラー: {response.status_code}\"})\n",
    "                continue\n",
    "\n",
    "            data = response.json()\n",
    "            course = data.get(\"ResultSet\", {}).get(\"Course\")\n",
    "            if isinstance(course, list):\n",
    "                course = course[0]\n",
    "\n",
    "            teiki = course.get(\"Teiki\", {})\n",
    "            displayRoute = teiki.get(\"DisplayRoute\", {})\n",
    "            route = course.get(\"Route\", {})\n",
    "            lines = route.get(\"Line\", [])\n",
    "            points = route.get(\"Point\", [])\n",
    "\n",
    "            timeOther = route.get(\"timeOther\", \"不明\")\n",
    "            timeOnBoard = route.get(\"timeOnBoard\", \"不明\")\n",
    "            timeWalk = route.get(\"timeWalk\", \"不明\")\n",
    "            distance = route.get(\"distance\", \"不明\")\n",
    "            transfer_count = route.get(\"transferCount\", \"不明\")\n",
    "\n",
    "            fare = \"不明\"\n",
    "            teiki1 = teiki3 = teiki6 = \"不明\"\n",
    "            for price in course.get(\"Price\", []):\n",
    "                kind = price.get(\"kind\") or price.get(\"Kind\")\n",
    "                if kind == \"FareSummary\":\n",
    "                    fare = price.get(\"Oneway\", \"不明\")\n",
    "                elif kind == \"Teiki1Summary\":\n",
    "                    teiki1 = price.get(\"Oneway\", \"不明\")\n",
    "                elif kind == \"Teiki3Summary\":\n",
    "                    teiki3 = price.get(\"Oneway\", \"不明\")\n",
    "                elif kind == \"Teiki6Summary\":\n",
    "                    teiki6 = price.get(\"Oneway\", \"不明\")\n",
    "\n",
    "            line_info = []\n",
    "            for i, line in enumerate(lines):\n",
    "                dep_idx = i\n",
    "                arr_idx = i + 1\n",
    "\n",
    "                dep_name = \"（出発地不明）\"\n",
    "                arr_name = \"（到着地不明）\"\n",
    "\n",
    "                if dep_idx < len(points):\n",
    "                    point_dep = points[dep_idx]\n",
    "                    dep_name = (\n",
    "                        point_dep.get(\"Station\", {}).get(\"Name\")\n",
    "                        or point_dep.get(\"Name\")\n",
    "                        or dep_name\n",
    "                    )\n",
    "\n",
    "                if arr_idx < len(points):\n",
    "                    point_arr = points[arr_idx]\n",
    "                    arr_name = (\n",
    "                        point_arr.get(\"Station\", {}).get(\"Name\")\n",
    "                        or point_arr.get(\"Name\")\n",
    "                        or arr_name\n",
    "                    )\n",
    "\n",
    "                transport_name = line.get(\"Name\") or line.get(\"TypicalName\")\n",
    "\n",
    "                if not transport_name or transport_name.strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                line_info.append({\n",
    "                    \"transport_name\": transport_name,\n",
    "                    \"from\": dep_name,\n",
    "                    \"to\": arr_name\n",
    "                })\n",
    "\n",
    "            result_data.append({\n",
    "                \"label\": label,\n",
    "                \"time_on_board\": timeOnBoard,\n",
    "                \"time_other\": timeOther,\n",
    "                \"time_walk\": timeWalk,\n",
    "                \"total_time\": int(timeOnBoard) + int(timeWalk) + int(timeOther),\n",
    "                \"distance\": distance,\n",
    "                \"transfer_count\": transfer_count,\n",
    "                \"fare\": fare,\n",
    "                \"teiki_1\": teiki1,\n",
    "                \"teiki_3\": teiki3,\n",
    "                \"teiki_6\": teiki6,\n",
    "                \"route\": line_info\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            result_data.append({\"label\": label, \"error\": f\"例外エラー: {e}\"})\n",
    "\n",
    "    return json.dumps(result_data, ensure_ascii=False)\n",
    "\n",
    "def extract_text_from_file(file_name: str) -> str:\n",
    "    file_path = f\"/Volumes/hhhd_demo_itec/commuting_allowance/inputs/{file_name}\"\n",
    "\n",
    "    try:\n",
    "        extracted_text = extract_text(file_path)\n",
    "        text = f\"\\n---\\n\\n### {os.path.basename(file_path)} の申請内容\\n\\n```\\n{extracted_text}\\n```\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: {file_path} の処理に失敗しました: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        print(\"ファイルの読み取りに失敗した、もしくは内容が空でした。\")\n",
    "        return \"ファイルの読み取りに失敗した、もしくは内容が空でした。\"\n",
    "\n",
    "    try:\n",
    "        # questionから情報を抽出して、定型文に変換するChain　\n",
    "        extract_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "            申請書データ:\n",
    "            {input_text}                                                      \n",
    "\n",
    "            上記の申請書データから、以下の情報を抽出してください：\n",
    "\n",
    "            - 申請者名\n",
    "            - 勤務先住所\n",
    "            - 自宅住所\n",
    "            - 最寄り駅\n",
    "            - 利用交通機関と経路\n",
    "            - 通勤距離\n",
    "            - 定期代\n",
    "\n",
    "            定期代が経路ごとに書かれている場合は、それぞれの定期代を足した総額を記載してください。\n",
    "            出力は以下のMarkdown形式に従ってください：\n",
    "\n",
    "            ---\n",
    "            ##申請書内容##\n",
    "            - **申請者名**:  \n",
    "            - **勤務先住所**:  \n",
    "            - **自宅住所**:  \n",
    "            - **最寄り駅**: \n",
    "            - **定期代**: \n",
    "            - **利用交通機関と経路①**:  \n",
    "            - **通勤距離①**:   \n",
    "            ...（以下続く）\n",
    "            ---\n",
    "            \"\"\")\n",
    "        model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "        extract_fields_chain = extract_prompt_template | model | StrOutputParser()\n",
    "\n",
    "        llm_text = extract_fields_chain.invoke({\"input_text\": text})\n",
    "    except Exception as e:\n",
    "        print(f\"LLMによる構造化抽出に失敗しました: {e}\")\n",
    "        return text.strip()\n",
    "\n",
    "    return llm_text\n",
    "\n",
    "def check_commuting_allowance_consistency(datas):\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "    - あなたは会社の通勤手当の申請が適正かどうかをチェックするシステムです。  \n",
    "    -【申請内容】を【会社規定】と照らし合わせて、申請が適切かどうかを判断してください。\n",
    "    -【過去の申請履歴】がある場合は、その**申請履歴を表示すること**。履歴がない場合は、申請履歴のセクションは表示しないこと。\n",
    "    -【最寄り駅と通勤ルートなどの検索結果】がある場合は、あわせて、判断すること。\n",
    "    - 経路が複数に分かれている場合は、定期代は**合計金額*で判断すること。また、通勤距離は**合計距離**で判断すること。\n",
    "    - 最終判断は、以下の3つから選び、**必ず理由と規定の引用を添えて出力すること。**\n",
    "    - **問題なし**：すべての規定に適合している場合。\n",
    "    - **問題あり**：一つでも規定に反している場合。\n",
    "    - **要確認**：規定に該当がない、判断できない、または別途基準が必要な場合。\n",
    "    ---\n",
    "\n",
    "    ### **出力フォーマット**\n",
    "\n",
    "    #### [申請者名] さん\n",
    "\n",
    "    - **過去の申請履歴**:  \n",
    "        - [申請履歴内容]  \n",
    "        - or `なし`\n",
    "\n",
    "    - **最寄り駅確認結果**:  \n",
    "    - `一致`  \n",
    "    - `不一致`　検索された最寄り駅：[検索された最寄り駅名]\n",
    "\n",
    "    - **会社規定の引用と判断理由**:  \n",
    "        - `[引用した会社規定]`  \n",
    "        - `[規程に基づいた詳細な判断理由]`\n",
    "\n",
    "    - **最終判断結果**:  \n",
    "    - `問題なし`  \n",
    "    - `問題あり`  \n",
    "    - `要確認`\n",
    "    \"\"\")\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "    ])\n",
    "    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)\n",
    "    extract_fields_chain = prompt | model | StrOutputParser()\n",
    "    result = extract_fields_chain.invoke({\"input\": datas})\n",
    "    return result\n",
    "\n",
    "def call_commuting_allowance_api_remand(data):\n",
    "    print(\"通勤手当申請差戻処理開始\")\n",
    "    return \"通勤手当申請差戻しました\"\n",
    "    \n",
    "def call_commuting_allowance_api_approval(data):\n",
    "    print(\"通勤手当申請承認処理開始\")\n",
    "    return \"通勤手当申請承認しました\"\n",
    "\n",
    "# args_schemas\n",
    "class TaxAdjustmentArgs(BaseModel):\n",
    "  adjustment_info: dict = Field(\n",
    "    description=\"年末調整申請情報。契約者名、保険会社名、保険名称、年金年、保険証券番号などを含むJSONオブジェクト形式。\"\n",
    "  )\n",
    "  image_info: dict = Field(\n",
    "    description=\"OCRによって抽出された保険証明書画像の内容。全文または項目別の情報を含むJSON形式。\"\n",
    "  )\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"search_company_regulations\",\n",
    "        func=search_company_regulations,\n",
    "        description=\"\"\"\n",
    "        会社規定を取得するツールです。\n",
    "        「会社規定を参照」と言われた場合は、このツールを使用してください。\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_tax_adjustment_history\",\n",
    "        func=get_tax_adjustment_history,\n",
    "        description=\"\"\"\n",
    "        通勤手当の過去の申請履歴を取得するツールです。\n",
    "        名前を使って、過去の申請履歴を取得します。\n",
    "        入力例: 「申請者の名前」\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_text_from_file\",\n",
    "        func=extract_text_from_file,\n",
    "        description=\"\"\"\n",
    "        申請書名を渡して、申請書内容を取得するツールです。\n",
    "        入力例: \"sample.pdf\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"search_commute_routes\",\n",
    "        func=search_commute_routes,\n",
    "        description= \"\"\"\n",
    "        JSON形式の文字列で渡された出発地と到着地の住所を使って、駅すぱあとAPIから最短・最速・最楽ルートを取得するツールです。\n",
    "        注意: 出発地と到着地は必ず都道府県から始まる完全な日本語住所にしてください。そして、ビル名などが要りません。\n",
    "        入力例: '{\"from_address\": \"大阪府大阪市中央区1-1-1\", \"to_address\": \"大阪府大阪市福島区1-2-3\"}'\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_commuting_allowance_api_remand\",\n",
    "        func=call_commuting_allowance_api_remand,\n",
    "        description=\"\"\"\n",
    "        通勤手当申請の差戻APIを呼び出すツールです。\n",
    "        申請内容を渡して、通勤手当申請の差戻APIを呼び出すと、差戻結果が返ります。\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_commuting_allowance_api_approval\",\n",
    "        func=call_commuting_allowance_api_approval,\n",
    "        description=\"\"\"\n",
    "        通勤手当申請の承認APIを呼び出すツールです。\n",
    "        申請内容を渡して、通勤手当申請の承認APIを呼び出すと、承認結果が返ります。\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"check_commuting_allowance_consistency\",\n",
    "        func=check_commuting_allowance_consistency,\n",
    "        description=\"\"\"\n",
    "        通勤手当申請が適正かどうかをチェックするツールです。\n",
    "\n",
    "        以下4つの引数を必ず指定してください：\n",
    "\n",
    "        - 申請内容: 通勤手当申請書の申請内容\n",
    "        - 会社規定: 通勤手当に関する会社規定\n",
    "        - 過去の申請履歴: ユーザーが過去申請した通勤手当の申請履歴\n",
    "        - 最寄り駅と通勤ルートなどの検索結果: 駅すぱあとAPIから取得した最短・最速・最楽ルートなどの検索結果\n",
    "\n",
    "        例：\n",
    "        {\n",
    "        \"申請内容\": { ... },\n",
    "        \"会社規定\": { ... },\n",
    "        \"過去の申請履歴\": { ... },\n",
    "        \"最寄り駅と通勤ルートなどの検索結果\": { ... },\n",
    "        }\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_tax_adjustment_info\",\n",
    "        func=get_tax_adjustment_info,\n",
    "        description=\"\"\"\n",
    "        \"ユーザーID\", \"保険区分\", \"保険内連番\"を使って、年末調整の申請情報を取得するツールです。\n",
    "        必ずダブルクォートで囲まれた **有効なJSON文字列** を入力してください。\n",
    "        入力例: \"{\\\"ユーザーID\\\": 0000001, \\\"保険区分\\\": \\\"XX保険\\\", \\\"保険内連番\\\": 1}\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ocr_tool\",\n",
    "        func=ocr_tool,\n",
    "        description=\"\"\"\n",
    "        画像名を渡して、画像情報を取得するツールです。\n",
    "        入力例: \"image.jpg\"\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_ehr_api_remand\",\n",
    "        func=call_ehr_api_remand,\n",
    "        description=\"\"\"\n",
    "        e-じんじの差戻APIを呼び出すツールです。\n",
    "        申請内容を渡して、e-じんじの差戻APIを呼び出すと、差戻結果が返ります。\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"call_ehr_api_approval\",\n",
    "        func=call_ehr_api_approval,\n",
    "        description=\"\"\"\n",
    "        e-じんじの承認APIを呼び出すツールです。\n",
    "        申請内容を渡して、e-じんじの承認APIを呼び出すと、承認結果が返ります。\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"check_tax_adjustment_consistency\",\n",
    "        func=check_tax_adjustment_consistency,\n",
    "        args_schemas=TaxAdjustmentArgs,\n",
    "        description=\"\"\"\n",
    "        年末調整申請内容と保険証明書（画像）のOCR結果を比較し、一致しているか確認します。\n",
    "\n",
    "        以下2つの引数を必ず指定してください：\n",
    "\n",
    "        - 申請情報: 年末調整の申請情報（辞書形式）\n",
    "        - 画像情報: OCRによって取得した画像情報（辞書形式）\n",
    "\n",
    "        例：\n",
    "        {\n",
    "        \"申請情報\": { ... },\n",
    "        \"画像情報\": { ... }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff9dd9c-3dc6-4f0b-9e5d-12f6126de9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "# Human-in-the-loop\n",
    "############################################\n",
    "# 人間参与\n",
    "def human_assistance(state: MyState):\n",
    "    print(\"### 管理者の確認待ち ###\")\n",
    "    # \"name\": \"check_tax_adjustment_consistency\n",
    "    last_query = state[\"messages\"][-2][\"content\"]\n",
    "    print(f\"\\n📩 エージェントの判断結果:\\n{last_query}\")\n",
    "\n",
    "    response_text = input(\"次のやるべきことを入力してください: \")\n",
    "\n",
    "    # tools_ran を更新（該当ツールを削除）\n",
    "    updated_tools_ran = set(state.get(\"tools_ran\", set()))\n",
    "    updated_tools_ran.discard(\"check_tax_adjustment_consistency\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": response_text,\n",
    "        \"id\": str(uuid.uuid4())\n",
    "        }],\n",
    "        \"tools_ran\": updated_tools_ran\n",
    "    }\n",
    "\n",
    "# ツールノード呼び出しカスタマイズ\n",
    "def custom_tool_node(state: dict) -> dict:\n",
    "    # ツールノード実行（LangGraph内部からの呼び出し）\n",
    "    tool_node = ChatAgentToolNode(tools)\n",
    "    tool_result_state = tool_node.invoke(state, config=None)\n",
    "\n",
    "    # 最新のメッセージ取得（AIMessage or dict）\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "\n",
    "    # tool_calls 取得（両対応：dict or AIMessage）\n",
    "    if isinstance(last_msg, dict):\n",
    "        tool_calls = last_msg.get(\"tool_calls\", [])\n",
    "    else:\n",
    "        tool_calls = getattr(last_msg, \"tool_calls\", [])\n",
    "\n",
    "    # すでに実行済みの tool のセット\n",
    "    tools_ran = set(state.get(\"tools_ran\", set()))\n",
    "\n",
    "    # 実行されたツール名を tools_ran に追加\n",
    "    for call in tool_calls:\n",
    "        # call 自体も dict または pydantic model の可能性があるため両対応\n",
    "        if isinstance(call, dict):\n",
    "            tool_name = call.get(\"name\")\n",
    "        else:\n",
    "            tool_name = getattr(call, \"name\", None)\n",
    "        if tool_name:\n",
    "            tools_ran.add(tool_name)\n",
    "\n",
    "    # 更新後の状態に tools_ran を追加\n",
    "    tool_result_state[\"tools_ran\"] = tools_ran\n",
    "    return tool_result_state\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "# LangGraphベースのエージェント構築\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    # モデルにツールをバインド\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    # 次の状態を判断する関数\n",
    "    def should_continue(state: MyState) -> str:\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        tools_ran = state.get(\"tools_ran\", set())\n",
    "        if \"tool_calls\" in last_msg and last_msg[\"tool_calls\"]:\n",
    "            function_name = last_msg[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "            if function_name == \"search_company_regulations\":\n",
    "                return \"vector_search\"\n",
    "            return \"tools\"\n",
    "        elif \"check_tax_adjustment_consistency\" in tools_ran:\n",
    "            tools_ran.discard(\"check_tax_adjustment_consistency\")\n",
    "            return \"human\"\n",
    "        elif \"check_commuting_allowance_consistency\" in tools_ran:\n",
    "            tools_ran.discard(\"check_commuting_allowance_consistency\")\n",
    "            return \"human\"\n",
    "        return \"end\"\n",
    "\n",
    "    # 前処理（入力された MyState から \"messages\" を取り出し、必要ならシステムプロンプトを先頭に追加）\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    # パイプライン定義\n",
    "    model_runnable = preprocessor | model\n",
    "    \n",
    "    def call_model(state: MyState, config: RunnableConfig):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        if response is None:\n",
    "            return {\"messages\": []}\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # 新しい状態stateを定義\n",
    "    workflow = StateGraph(MyState)\n",
    "    # nodeを追加\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", RunnableLambda(custom_tool_node))\n",
    "    workflow.add_node(\"human\", RunnableLambda(human_assistance))\n",
    "    workflow.add_node(\"vector_search\", RunnableLambda(vector_search))\n",
    "\n",
    "    ###########################\n",
    "    ## 状態遷移ルールを定義\n",
    "    ## agent → \"tool_calls\"あり → tools\n",
    "    ## tools → 呼び出し後 → agent に戻る（再応答）\n",
    "    ###########################\n",
    "    # 入口\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    # 条件edge追加\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"human\": \"human\",\n",
    "            \"vector_search\": \"vector_search\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # 一般edge追加\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    workflow.add_edge(\"human\", \"agent\")\n",
    "    workflow.add_edge(\"vector_search\", \"tools\")\n",
    "    # メモリを追加\n",
    "    memory = MemorySaver()\n",
    "    # コンパイル\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "# 出力結果の形を整形する\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        config = {\"configurable\": {\"thread_id\": context.thread_id if context else \"session_001\"}}\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, config=config, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        config = {\"configurable\": {\"thread_id\": context.thread_id if context else \"session_001\"}}\n",
    "        for event in self.agent.stream(request, config=config, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)\n",
    "\n",
    "# 流れ図生成\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     print(\"Failed to render graph.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agent.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
