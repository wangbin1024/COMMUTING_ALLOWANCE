from typing import Any, Generator, Optional, Sequence, Union, TypedDict, Literal, Annotated, Union, List, Dict
from pydantic import BaseModel, Field
from pyspark.sql import SparkSession
from databricks_langchain import ChatDatabricks, UCFunctionToolkit, VectorSearchRetrieverTool
from databricks.vector_search.client import VectorSearchClient
from databricks_langchain import DatabricksVectorSearch, ChatDatabricks
# import mlflow
from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode
from mlflow.pyfunc import ChatAgent
from mlflow.types.agent import ChatAgentChunk, ChatAgentMessage, ChatAgentResponse, ChatContext
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.language_models import LanguageModelLike
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.runnables import RunnableConfig, RunnableLambda
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import BaseTool, Tool
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langgraph.graph import END, StateGraph 
from langgraph.types import Command, interrupt
from langgraph.graph.graph import CompiledGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt.tool_node import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from google.cloud import vision
from databricks import sql
from pypdf import PdfReader
from docx import Document
from datetime import date, datetime
import json
import requests
import pandas as pd
import re
import math
import os
import operator
import uuid

from langchain.callbacks import LangChainTracer

LANGSMITH_API_KEY = "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"
LANGSMITH_PROJECT = "agent-eval-project"

os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_API_KEY
os.environ["LANGCHAIN_PROJECT"] = LANGSMITH_PROJECT

tracer = LangChainTracer()

############################################
# LLM endpoint
############################################
LLM_ENDPOINT_NAME_1 = "databricks-meta-llama-3-3-70b-instruct"
LLM_ENDPOINT_NAME_2 = "databricks-claude-3-7-sonnet"
llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)

############################################
# system prompt
############################################
system_prompt = """
ã‚ãªãŸã¯æ¥­å‹™æ”¯æ´ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
å¿…è¦ã«å¿œã˜ã¦é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚„æŒ‡ç¤ºã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚
"""

# çŠ¶æ…‹ç®¡ç†ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã€€ChatAgentStateæ‹¡å¼µ
class MyState(ChatAgentState):
  tools_ran: Annotated[set[str], operator.or_]

# COMMAND ----------

# utilsé–¢æ•°
def convert_dates(obj):
  if isinstance(obj, (date, datetime)):
    return obj.isoformat()
  return obj

def safe_json(obj):
  if isinstance(obj, list):
    return [safe_json(item) for item in obj]
  elif isinstance(obj, dict):
    return {k: safe_json(v) for k, v in obj.items()}
  else:
    return convert_dates(obj)

def format_context(docs):
  chunk_template = (
    "{chunk_text}\n\n"
  )
  chunk_contents = [
    chunk_template.format(
      index=i + 1,
      chunk_text=d.page_content.strip()
    )
    for i, d in enumerate(docs)
  ]

  contents = "".join(chunk_contents)
  return contents

def vector_search(state: MyState) -> str:
    question = state["messages"][-2]["content"]
    print(f"\nğŸ“© vector_search")
    # Connect to the Vector Search Index
    vs_client = VectorSearchClient(disable_notice=True)

    VECTOR_SEARCH_ENDPOINT = 'commuting_allowance_vector_search'
    VECTOR_SEARCH_INDEX = 'hhhd_demo_itec.commuting_allowance.commuting_allowance_index'

    # LangChain retrieverã®ä½œæˆ
    embedding_model = HuggingFaceEmbeddings(
        model_name="/Workspace/Users/wang-b2@itec.hankyu-hanshin.co.jp/ruri-base-v2"
    )
    vector_search_as_retriever = DatabricksVectorSearch(
        endpoint=VECTOR_SEARCH_ENDPOINT,
        index_name=VECTOR_SEARCH_INDEX,
        embedding=embedding_model,
        text_column="chunked_text",
        columns=["chunk_id", "chunked_text"]
    ).as_retriever(search_kwargs={"k": 8, "score_threshold": 0.7})

    # result = vector_search_as_retriever.invoke(question)
    result = vector_search_as_retriever.get_relevant_documents(question)
    if len(result) > 0:
        context = format_context(result)
        tool_call = state["messages"][-1]["tool_calls"][0]
        arguments = json.loads(tool_call["function"]["arguments"])
        if "__arg1" in arguments:
            arguments["__arg1"] = context
            
        tool_call["function"]["arguments"] = json.dumps(arguments, ensure_ascii=False)
        return state
    return state

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, "rb") as f:
        reader = PdfReader(f)
        text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
    return text

def extract_text_from_docx(docx_path):
    doc = Document(docx_path)
    text = ""

    # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ®µè½ï¼‰
    for para in doc.paragraphs:
        if para.text.strip():  # ç©ºç™½è¡Œã‚’å‰Šé™¤
            text += para.text.strip() + "\n"

    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    for table in doc.tables:
        processed_cells = set()  # æ—¢ã«å‡¦ç†ã—ãŸã‚»ãƒ«ã‚’ä¿å­˜

        for row in table.rows:
            row_data = []
            for cell in row.cells:
                cell_text = cell.text.strip()
                if (cell._tc not in processed_cells) and cell_text:  # çµåˆã‚»ãƒ«ã‚’é™¤å¤–
                    row_data.append(cell_text)
                    processed_cells.add(cell._tc)
                else:
                    row_data.append("")

            text += " | ".join(row_data) + "\n"

    return text

def extract_text_from_xlsx(xlsx_path):
    df = pd.read_excel(xlsx_path, engine='openpyxl', sheet_name=None) 
    text = ""
    for sheet_name, sheet_df in df.items():
        text += f"### {sheet_name} ã‚·ãƒ¼ãƒˆ ###\n"
        text += sheet_df.to_string(index=False, header=True) + "\n"
    return text

def extract_text_from_txt(txt_path):
    with open(txt_path, "r", encoding="shift_jis") as f:
        text = f.read()
    return text

def extract_text(file_path):
    text = os.path.splitext(file_path)[1].lower()

    if text == ".pdf":
        return extract_text_from_pdf(file_path)
    elif text == ".docx":
        return extract_text_from_docx(file_path)
    elif text in [".xls", ".xlsx"]:
        return extract_text_from_xlsx(file_path)
    elif text == ".txt":
        return extract_text_from_txt(file_path)
    else:
        raise ValueError(f"å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™: {text}")

def create_llm_agent(model, tools, prompt):
  agent = create_tool_calling_agent(model, tools, prompt)
  return AgentExecutor(agent=agent, tools=tools, verbose=True)

# COMMAND ----------

############################################
# tools
############################################
def get_tax_adjustment_info(datas):
  try:
    # JSONã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    data = json.loads(datas)
    user_id = data["ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"]
    insurance_classification = data["ä¿é™ºåŒºåˆ†"]
    insurance_serial_number = data["ä¿é™ºå†…é€£ç•ª"]

    spark = SparkSession.builder.getOrCreate()

    query = f"""
    SELECT * FROM hhhd_demo_itec.tax_adjustment.`ä¿é™ºæ–™æ˜ç´°æƒ…å ±` 
    WHERE `ç”³è«‹çŠ¶æ³ï¼ˆä¿é™ºæ–™ï¼‰` <> 'å®Œäº†' AND `ãƒ¦ãƒ¼ã‚¶ãƒ¼ID` = '{user_id}' AND `ä¿é™ºåŒºåˆ†` = '{insurance_classification}'  
    """

    if insurance_serial_number:
        query += f" AND `ä¿é™ºå†…é€£ç•ª` = '{insurance_serial_number}'"

    df = spark.sql(query)
    result = df.toPandas().to_dict(orient="records")

    print(f"ç”³è«‹æƒ…å ±result: {result}")

    return json.dumps({
      "ç”³è«‹æƒ…å ±": safe_json(result[0]) if result else {},
      "ä»¶æ•°": len(result)
    }, ensure_ascii=False)

  except Exception as e:
    return json.dumps({
      "ã‚¨ãƒ©ãƒ¼": f"ç”³è«‹æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    }, ensure_ascii=False)

def ocr_tool(image_name):
    try:
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’æŒ‡å®š
        spark = SparkSession.builder.getOrCreate()
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/Volumes/hhhd_demo_itec/tax_adjustment/keys/directed-reef-454701-d6-0f8e0d1de3c1.json"

        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
        client = vision.ImageAnnotatorClient()

        # OCRå¯¾è±¡ç”»åƒã®èª­ã¿è¾¼ã¿
        image_path = f"/Volumes/hhhd_demo_itec/tax_adjustment/images/{image_name}"
        with open(image_path, "rb") as image_file:
            content = image_file.read()
            image = vision.Image(content=content)

        # OCRå®Ÿè¡Œï¼ˆæ—¥æœ¬èªå«ã‚€ï¼‰
        response = client.text_detection(image=image)
        texts = response.text_annotations

        if response.error.message:
            raise Exception(f"APIã‚¨ãƒ©ãƒ¼: {response.error.message}")

        if texts:
            vision_text = texts[0].description
            return json.dumps({
                "ç”»åƒæƒ…å ±": {
                "ocrå…¨æ–‡": vision_text.strip()
                }
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "ç”»åƒæƒ…å ±": {
                "ocrå…¨æ–‡": "",
                "æ³¨æ„": "ãƒ†ã‚­ã‚¹ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
                }
            }, ensure_ascii=False)

    except Exception as e:
        return json.dumps({
        "ã‚¨ãƒ©ãƒ¼": f"OCRå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        }, ensure_ascii=False)

def check_tax_adjustment_consistency(datas):
    system_prompt = SystemMessagePromptTemplate.from_template("""
    ã‚ãªãŸã¯å¹´æœ«èª¿æ•´ã®ç”³è«‹å†…å®¹ã¨ã€OCRã§èª­ã¿å–ã£ãŸä¿é™ºæ–™æ§é™¤è¨¼æ˜æ›¸ç”»åƒã®æƒ…å ±ã‚’ç…§åˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ç”Ÿå‘½ä¿é™ºã®å ´åˆã¯ä¿é™ºåˆ†é¡ãŒã‚ã‚Šã¾ã™ã€‚åŒã˜ä¿é™ºåˆ†é¡ã®æƒ…å ±ã‚’ç…§åˆã—ã¦ãã ã•ã„ã€‚
    ä¿é™ºæ–™æ§é™¤è¨¼æ˜ã«ã¯11æœˆåˆ†ã¨12æœˆåˆ†ãŒåˆ†ã‹ã‚Œã¦è¨˜è¼‰ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã®å ´åˆã¯ç”³å‘Šé¡ã‚’åˆç®—ã—ã¦ãã ã•ã„ã€‚
    ä¿é™ºæ–™æ§é™¤è¨¼æ˜æ›¸ã«è¤‡æ•°ç”³å‘Šå¹´ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€åŒã˜ç”³å‘Šå¹´ã®æƒ…å ±ã‚’ç…§åˆã—ã¦ãã ã•ã„ã€‚

    ã€ã‚¿ã‚¹ã‚¯ã€‘
    ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦ã€ç”³è«‹æƒ…å ±ã¨ç”»åƒæƒ…å ±ã‚’æ¯”è¼ƒã—ã€ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
    - å¥‘ç´„è€…å
    - ä¿é™ºä¼šç¤¾ãƒ»äº‹æ¥­æ‰€å
    - ä¿é™ºåç§°
    - ä¿é™ºæœŸé–“
    - ç”Ÿå‘½ä¿é™ºåˆ†é¡ï¼ˆç”Ÿå‘½ä¿é™ºã®å ´åˆã®ã¿ï¼‰
    - ä¿é™ºç¨®é¡
    - ç”³å‘Šé¡
    - ç”³å‘Šå¹´

    ã€å‡ºåŠ›å½¢å¼ã€‘
    ä»¥ä¸‹ã®ã‚ˆã†ãªèª­ã¿ã‚„ã™ã„å½¢å¼ã§ã€æ¯”è¼ƒçµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ï¼š

    ---
    ## å¥‘ç´„è€…åï¼š
    - ç”³è«‹æƒ…å ±ï¼šXX å¤ªéƒ
    - ç”»åƒæƒ…å ±ï¼šXX å¤ªéƒ
    - ä¸€è‡´

    ## ä¿é™ºåç§°ï¼š
    - ç”³è«‹æƒ…å ±ï¼šXXä¿é™º
    - ç”»åƒæƒ…å ±ï¼šOOä¿é™º
    - ä¸ä¸€è‡´

    ## ç·åˆè©•ä¾¡ï¼š
    ---
    """)
    user_prompt = HumanMessagePromptTemplate.from_template("{input}")
    prompt = ChatPromptTemplate.from_messages([
        system_prompt,
        user_prompt,
    ])
    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)
    extract_fields_chain = prompt | model | StrOutputParser()
    result = extract_fields_chain.invoke({"input": datas})
    return result

def call_ehr_api_remand(data):
    print("å·®æˆ»å‡¦ç†é–‹å§‹")
    # response = requests.get("https://api.hhhd.jp/ehr/remand")

    # if response.error.message:
    #     raise Exception(f"APIã‚¨ãƒ©ãƒ¼: {response.error.message}")
    # else:
    #     return response.text
    return "å·®æˆ»ã—ã¾ã—ãŸ"
    
def call_ehr_api_approval(data):
    print("æ‰¿èªå‡¦ç†é–‹å§‹")
    # response = requests.get("https://api.hhhd.jp/ehr/approval")

    # if response.error.message:
    #     raise Exception(f"APIã‚¨ãƒ©ãƒ¼: {response.error.message}")
    # else:
    #     return response.text
    return "æ‰¿èªã—ã¾ã—ãŸ"

def search_company_regulations(question: str) -> str:
    return f"ä¼šç¤¾è¦å®š:\n\n{question}"

def get_tax_adjustment_history(name: str) -> str:
    spark = SparkSession.builder.getOrCreate()
    clean_name = name.replace(" ", "")
    if not clean_name:
        print("åå‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    query = f"""
    SELECT name, work_address, address, nearest_station, 
            route_1, distance_1, commuter_pass_1, 
            route_2, distance_2, commuter_pass_2
    FROM hhhd_demo_itec.allowance_payment_rules.commuting_allowance_history
    WHERE name = '{clean_name}'
    """

    try:
        df = spark.sql(query)
        text = json.dumps(df.collect(), ensure_ascii=False)
        return f"éå»ã®ç”³è«‹å±¥æ­´:\n\n{text}"
    except Exception as e:
        print(f"SQLå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return "ç”³è«‹å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
 
def search_commute_routes(requests_data: str) -> str:
    try:
        data = json.loads(requests_data)
    except Exception as e:
        return json.dumps({"error": f"JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"})

    api_key = "test_nfY87YJYHMp"
    base_url = "https://api.ekispert.jp/v1/json/search/course/extreme"

    from_address = data.get("from_address")
    to_address = data.get("to_address")
    
    if not from_address or not to_address:
        return json.dumps({"error": "ã‚¨ãƒ©ãƒ¼: å‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã®ä½æ‰€ã¯å¿…é ˆã§ã™ã€‚"})
    print(f"from_address: {from_address} to_address: {to_address}")
    radius = 1000
    result_data = []

    sort_types = {
        "time": "æœ€çŸ­ãƒ«ãƒ¼ãƒˆ",
        "teiki": "æœ€å®‰ãƒ«ãƒ¼ãƒˆ",
        "transfer": "ä¹—æ›å°‘ãªã„ãƒ«ãƒ¼ãƒˆ"
    }

    for sort_key, label in sort_types.items():
        params = {
            "key": api_key,
            "viaList": f"{from_address},{radius}:{to_address},{radius}",
            "searchType": "plain",
            "sort": sort_key,
            "answerCount": 10,
        }

        try:
            response = requests.get(base_url, params=params)
            if response.status_code != 200:
                result_data.append({"label": label, "error": f"APIã‚¨ãƒ©ãƒ¼: {response.status_code}"})
                continue

            data = response.json()
            course = data.get("ResultSet", {}).get("Course")
            if isinstance(course, list):
                course = course[0]

            teiki = course.get("Teiki", {})
            displayRoute = teiki.get("DisplayRoute", {})
            route = course.get("Route", {})
            lines = route.get("Line", [])
            points = route.get("Point", [])

            timeOther = route.get("timeOther", "ä¸æ˜")
            timeOnBoard = route.get("timeOnBoard", "ä¸æ˜")
            timeWalk = route.get("timeWalk", "ä¸æ˜")
            distance = route.get("distance", "ä¸æ˜")
            transfer_count = route.get("transferCount", "ä¸æ˜")

            fare = "ä¸æ˜"
            teiki1 = teiki3 = teiki6 = "ä¸æ˜"
            for price in course.get("Price", []):
                kind = price.get("kind") or price.get("Kind")
                if kind == "FareSummary":
                    fare = price.get("Oneway", "ä¸æ˜")
                elif kind == "Teiki1Summary":
                    teiki1 = price.get("Oneway", "ä¸æ˜")
                elif kind == "Teiki3Summary":
                    teiki3 = price.get("Oneway", "ä¸æ˜")
                elif kind == "Teiki6Summary":
                    teiki6 = price.get("Oneway", "ä¸æ˜")

            line_info = []
            for i, line in enumerate(lines):
                dep_idx = i
                arr_idx = i + 1

                dep_name = "ï¼ˆå‡ºç™ºåœ°ä¸æ˜ï¼‰"
                arr_name = "ï¼ˆåˆ°ç€åœ°ä¸æ˜ï¼‰"

                if dep_idx < len(points):
                    point_dep = points[dep_idx]
                    dep_name = (
                        point_dep.get("Station", {}).get("Name")
                        or point_dep.get("Name")
                        or dep_name
                    )

                if arr_idx < len(points):
                    point_arr = points[arr_idx]
                    arr_name = (
                        point_arr.get("Station", {}).get("Name")
                        or point_arr.get("Name")
                        or arr_name
                    )

                transport_name = line.get("Name") or line.get("TypicalName")

                if not transport_name or transport_name.strip() == "":
                    continue

                line_info.append({
                    "transport_name": transport_name,
                    "from": dep_name,
                    "to": arr_name
                })

            result_data.append({
                "label": label,
                "time_on_board": timeOnBoard,
                "time_other": timeOther,
                "time_walk": timeWalk,
                "total_time": int(timeOnBoard) + int(timeWalk) + int(timeOther),
                "distance": distance,
                "transfer_count": transfer_count,
                "fare": fare,
                "teiki_1": teiki1,
                "teiki_3": teiki3,
                "teiki_6": teiki6,
                "route": line_info
            })

        except Exception as e:
            result_data.append({"label": label, "error": f"ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {e}"})

    return json.dumps(result_data, ensure_ascii=False)

def extract_text_from_file(file_name: str) -> str:
    file_path = f"/Volumes/hhhd_demo_itec/commuting_allowance/inputs/{file_name}"

    try:
        extracted_text = extract_text(file_path)
        text = f"\n---\n\n### {os.path.basename(file_path)} ã®ç”³è«‹å†…å®¹\n\n```\n{extracted_text}\n```\n"
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return ""

    if not text.strip():
        print("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ãŸã€ã‚‚ã—ãã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚")
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ãŸã€ã‚‚ã—ãã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚"

    try:
        # questionã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã€å®šå‹æ–‡ã«å¤‰æ›ã™ã‚‹Chainã€€
        extract_prompt_template = PromptTemplate.from_template("""
            ç”³è«‹æ›¸ãƒ‡ãƒ¼ã‚¿:
            {input_text}                                                      

            ä¸Šè¨˜ã®ç”³è«‹æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

            - ç”³è«‹è€…å
            - å‹¤å‹™å…ˆä½æ‰€
            - è‡ªå®…ä½æ‰€
            - æœ€å¯„ã‚Šé§…
            - åˆ©ç”¨äº¤é€šæ©Ÿé–¢ã¨çµŒè·¯
            - é€šå‹¤è·é›¢
            - å®šæœŸä»£

            å®šæœŸä»£ãŒçµŒè·¯ã”ã¨ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã®å®šæœŸä»£ã‚’è¶³ã—ãŸç·é¡ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
            å‡ºåŠ›ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

            ---
            ##ç”³è«‹æ›¸å†…å®¹##
            - **ç”³è«‹è€…å**:  
            - **å‹¤å‹™å…ˆä½æ‰€**:  
            - **è‡ªå®…ä½æ‰€**:  
            - **æœ€å¯„ã‚Šé§…**: 
            - **å®šæœŸä»£**: 
            - **åˆ©ç”¨äº¤é€šæ©Ÿé–¢ã¨çµŒè·¯â‘ **:  
            - **é€šå‹¤è·é›¢â‘ **:   
            ...ï¼ˆä»¥ä¸‹ç¶šãï¼‰
            ---
            """)
        model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)
        extract_fields_chain = extract_prompt_template | model | StrOutputParser()

        llm_text = extract_fields_chain.invoke({"input_text": text})
    except Exception as e:
        print(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return text.strip()

    return llm_text

def check_commuting_allowance_consistency(datas):
    system_prompt = SystemMessagePromptTemplate.from_template("""
    - ã‚ãªãŸã¯ä¼šç¤¾ã®é€šå‹¤æ‰‹å½“ã®ç”³è«‹ãŒé©æ­£ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚  
    -ã€ç”³è«‹å†…å®¹ã€‘ã‚’ã€ä¼šç¤¾è¦å®šã€‘ã¨ç…§ã‚‰ã—åˆã‚ã›ã¦ã€ç”³è«‹ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
    -ã€éå»ã®ç”³è«‹å±¥æ­´ã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ãã®**ç”³è«‹å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨**ã€‚å±¥æ­´ãŒãªã„å ´åˆã¯ã€ç”³è«‹å±¥æ­´ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è¡¨ç¤ºã—ãªã„ã“ã¨ã€‚
    -ã€æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ã‚ã‚ã›ã¦ã€åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚
    - çµŒè·¯ãŒè¤‡æ•°ã«åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å®šæœŸä»£ã¯**åˆè¨ˆé‡‘é¡*ã§åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚ã¾ãŸã€é€šå‹¤è·é›¢ã¯**åˆè¨ˆè·é›¢**ã§åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚
    - æœ€çµ‚åˆ¤æ–­ã¯ã€ä»¥ä¸‹ã®3ã¤ã‹ã‚‰é¸ã³ã€**å¿…ãšç†ç”±ã¨è¦å®šã®å¼•ç”¨ã‚’æ·»ãˆã¦å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚**
    - **å•é¡Œãªã—**ï¼šã™ã¹ã¦ã®è¦å®šã«é©åˆã—ã¦ã„ã‚‹å ´åˆã€‚
    - **å•é¡Œã‚ã‚Š**ï¼šä¸€ã¤ã§ã‚‚è¦å®šã«åã—ã¦ã„ã‚‹å ´åˆã€‚
    - **è¦ç¢ºèª**ï¼šè¦å®šã«è©²å½“ãŒãªã„ã€åˆ¤æ–­ã§ããªã„ã€ã¾ãŸã¯åˆ¥é€”åŸºæº–ãŒå¿…è¦ãªå ´åˆã€‚
    ---

    ### **å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**

    #### [ç”³è«‹è€…å] ã•ã‚“

    - **éå»ã®ç”³è«‹å±¥æ­´**:  
        - [ç”³è«‹å±¥æ­´å†…å®¹]  
        - or `ãªã—`

    - **æœ€å¯„ã‚Šé§…ç¢ºèªçµæœ**:  
    - `ä¸€è‡´`  
    - `ä¸ä¸€è‡´`ã€€æ¤œç´¢ã•ã‚ŒãŸæœ€å¯„ã‚Šé§…ï¼š[æ¤œç´¢ã•ã‚ŒãŸæœ€å¯„ã‚Šé§…å]

    - **ä¼šç¤¾è¦å®šã®å¼•ç”¨ã¨åˆ¤æ–­ç†ç”±**:  
        - `[å¼•ç”¨ã—ãŸä¼šç¤¾è¦å®š]`  
        - `[è¦ç¨‹ã«åŸºã¥ã„ãŸè©³ç´°ãªåˆ¤æ–­ç†ç”±]`

    - **æœ€çµ‚åˆ¤æ–­çµæœ**:  
    - `å•é¡Œãªã—`  
    - `å•é¡Œã‚ã‚Š`  
    - `è¦ç¢ºèª`
    """)
    user_prompt = HumanMessagePromptTemplate.from_template("{input}")
    prompt = ChatPromptTemplate.from_messages([
        system_prompt,
        user_prompt,
    ])
    model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME_2, temperature=0.0)
    extract_fields_chain = prompt | model | StrOutputParser()
    result = extract_fields_chain.invoke({"input": datas})
    return result

def call_commuting_allowance_api_remand(data):
    print("é€šå‹¤æ‰‹å½“ç”³è«‹å·®æˆ»å‡¦ç†é–‹å§‹")
    return "é€šå‹¤æ‰‹å½“ç”³è«‹å·®æˆ»ã—ã¾ã—ãŸ"
    
def call_commuting_allowance_api_approval(data):
    print("é€šå‹¤æ‰‹å½“ç”³è«‹æ‰¿èªå‡¦ç†é–‹å§‹")
    return "é€šå‹¤æ‰‹å½“ç”³è«‹æ‰¿èªã—ã¾ã—ãŸ"

# args_schemas
class TaxAdjustmentArgs(BaseModel):
  adjustment_info: dict = Field(
    description="å¹´æœ«èª¿æ•´ç”³è«‹æƒ…å ±ã€‚å¥‘ç´„è€…åã€ä¿é™ºä¼šç¤¾åã€ä¿é™ºåç§°ã€å¹´é‡‘å¹´ã€ä¿é™ºè¨¼åˆ¸ç•ªå·ãªã©ã‚’å«ã‚€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã€‚"
  )
  image_info: dict = Field(
    description="OCRã«ã‚ˆã£ã¦æŠ½å‡ºã•ã‚ŒãŸä¿é™ºè¨¼æ˜æ›¸ç”»åƒã®å†…å®¹ã€‚å…¨æ–‡ã¾ãŸã¯é …ç›®åˆ¥ã®æƒ…å ±ã‚’å«ã‚€JSONå½¢å¼ã€‚"
  )

tools = [
    Tool(
        name="search_company_regulations",
        func=search_company_regulations,
        description="""
        ä¼šç¤¾è¦å®šã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        ã€Œä¼šç¤¾è¦å®šã‚’å‚ç…§ã€ã¨è¨€ã‚ã‚ŒãŸå ´åˆã¯ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """
    ),
    Tool(
        name="get_tax_adjustment_history",
        func=get_tax_adjustment_history,
        description="""
        é€šå‹¤æ‰‹å½“ã®éå»ã®ç”³è«‹å±¥æ­´ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        åå‰ã‚’ä½¿ã£ã¦ã€éå»ã®ç”³è«‹å±¥æ­´ã‚’å–å¾—ã—ã¾ã™ã€‚
        å…¥åŠ›ä¾‹: ã€Œç”³è«‹è€…ã®åå‰ã€
        """
    ),
    Tool(
        name="extract_text_from_file",
        func=extract_text_from_file,
        description="""
        ç”³è«‹æ›¸åã‚’æ¸¡ã—ã¦ã€ç”³è«‹æ›¸å†…å®¹ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        å…¥åŠ›ä¾‹: "sample.pdf"
        """
    ),
    Tool(
        name="search_commute_routes",
        func=search_commute_routes,
        description= """
        JSONå½¢å¼ã®æ–‡å­—åˆ—ã§æ¸¡ã•ã‚ŒãŸå‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã®ä½æ‰€ã‚’ä½¿ã£ã¦ã€é§…ã™ã±ã‚ã¨APIã‹ã‚‰æœ€çŸ­ãƒ»æœ€é€Ÿãƒ»æœ€æ¥½ãƒ«ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        æ³¨æ„: å‡ºç™ºåœ°ã¨åˆ°ç€åœ°ã¯å¿…ãšéƒ½é“åºœçœŒã‹ã‚‰å§‹ã¾ã‚‹å®Œå…¨ãªæ—¥æœ¬èªä½æ‰€ã«ã—ã¦ãã ã•ã„ã€‚ãã—ã¦ã€ãƒ“ãƒ«åãªã©ãŒè¦ã‚Šã¾ã›ã‚“ã€‚
        å…¥åŠ›ä¾‹: '{"from_address": "å¤§é˜ªåºœå¤§é˜ªå¸‚ä¸­å¤®åŒº1-1-1", "to_address": "å¤§é˜ªåºœå¤§é˜ªå¸‚ç¦å³¶åŒº1-2-3"}'
        """
    ),
    Tool(
        name="call_commuting_allowance_api_remand",
        func=call_commuting_allowance_api_remand,
        description="""
        é€šå‹¤æ‰‹å½“ç”³è«‹ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€é€šå‹¤æ‰‹å½“ç”³è«‹ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ã¨ã€å·®æˆ»çµæœãŒè¿”ã‚Šã¾ã™ã€‚
        """
    ),
    Tool(
        name="call_commuting_allowance_api_approval",
        func=call_commuting_allowance_api_approval,
        description="""
        é€šå‹¤æ‰‹å½“ç”³è«‹ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€é€šå‹¤æ‰‹å½“ç”³è«‹ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ã¨ã€æ‰¿èªçµæœãŒè¿”ã‚Šã¾ã™ã€‚
        """
    ),
    Tool(
        name="check_commuting_allowance_consistency",
        func=check_commuting_allowance_consistency,
        description="""
        é€šå‹¤æ‰‹å½“ç”³è«‹ãŒé©æ­£ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

        ä»¥ä¸‹4ã¤ã®å¼•æ•°ã‚’å¿…ãšæŒ‡å®šã—ã¦ãã ã•ã„ï¼š

        - ç”³è«‹å†…å®¹: é€šå‹¤æ‰‹å½“ç”³è«‹æ›¸ã®ç”³è«‹å†…å®¹
        - ä¼šç¤¾è¦å®š: é€šå‹¤æ‰‹å½“ã«é–¢ã™ã‚‹ä¼šç¤¾è¦å®š
        - éå»ã®ç”³è«‹å±¥æ­´: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ç”³è«‹ã—ãŸé€šå‹¤æ‰‹å½“ã®ç”³è«‹å±¥æ­´
        - æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ: é§…ã™ã±ã‚ã¨APIã‹ã‚‰å–å¾—ã—ãŸæœ€çŸ­ãƒ»æœ€é€Ÿãƒ»æœ€æ¥½ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ

        ä¾‹ï¼š
        {
        "ç”³è«‹å†…å®¹": { ... },
        "ä¼šç¤¾è¦å®š": { ... },
        "éå»ã®ç”³è«‹å±¥æ­´": { ... },
        "æœ€å¯„ã‚Šé§…ã¨é€šå‹¤ãƒ«ãƒ¼ãƒˆãªã©ã®æ¤œç´¢çµæœ": { ... },
        }
        """
    ),
    Tool(
        name="get_tax_adjustment_info",
        func=get_tax_adjustment_info,
        description="""
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ID", "ä¿é™ºåŒºåˆ†", "ä¿é™ºå†…é€£ç•ª"ã‚’ä½¿ã£ã¦ã€å¹´æœ«èª¿æ•´ã®ç”³è«‹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        å¿…ãšãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚ŒãŸ **æœ‰åŠ¹ãªJSONæ–‡å­—åˆ—** ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
        å…¥åŠ›ä¾‹: "{\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID\": 0000001, \"ä¿é™ºåŒºåˆ†\": \"XXä¿é™º\", \"ä¿é™ºå†…é€£ç•ª\": 1}"
        """
    ),
    Tool(
        name="ocr_tool",
        func=ocr_tool,
        description="""
        ç”»åƒåã‚’æ¸¡ã—ã¦ã€ç”»åƒæƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        å…¥åŠ›ä¾‹: "image.jpg"
        """
    ),
    Tool(
        name="call_ehr_api_remand",
        func=call_ehr_api_remand,
        description="""
        e-ã˜ã‚“ã˜ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€e-ã˜ã‚“ã˜ã®å·®æˆ»APIã‚’å‘¼ã³å‡ºã™ã¨ã€å·®æˆ»çµæœãŒè¿”ã‚Šã¾ã™ã€‚
        """
    ),
    Tool(
        name="call_ehr_api_approval",
        func=call_ehr_api_approval,
        description="""
        e-ã˜ã‚“ã˜ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        ç”³è«‹å†…å®¹ã‚’æ¸¡ã—ã¦ã€e-ã˜ã‚“ã˜ã®æ‰¿èªAPIã‚’å‘¼ã³å‡ºã™ã¨ã€æ‰¿èªçµæœãŒè¿”ã‚Šã¾ã™ã€‚
        """
    ),
    Tool(
        name="check_tax_adjustment_consistency",
        func=check_tax_adjustment_consistency,
        args_schemas=TaxAdjustmentArgs,
        description="""
        å¹´æœ«èª¿æ•´ç”³è«‹å†…å®¹ã¨ä¿é™ºè¨¼æ˜æ›¸ï¼ˆç”»åƒï¼‰ã®OCRçµæœã‚’æ¯”è¼ƒã—ã€ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚

        ä»¥ä¸‹2ã¤ã®å¼•æ•°ã‚’å¿…ãšæŒ‡å®šã—ã¦ãã ã•ã„ï¼š

        - ç”³è«‹æƒ…å ±: å¹´æœ«èª¿æ•´ã®ç”³è«‹æƒ…å ±ï¼ˆè¾æ›¸å½¢å¼ï¼‰
        - ç”»åƒæƒ…å ±: OCRã«ã‚ˆã£ã¦å–å¾—ã—ãŸç”»åƒæƒ…å ±ï¼ˆè¾æ›¸å½¢å¼ï¼‰

        ä¾‹ï¼š
        {
        "ç”³è«‹æƒ…å ±": { ... },
        "ç”»åƒæƒ…å ±": { ... }
        }
        """
    )
]

# COMMAND ----------


############################################
# Human-in-the-loop
############################################
# äººé–“å‚ä¸
def human_assistance(state: MyState):
    print("### ç®¡ç†è€…ã®ç¢ºèªå¾…ã¡ ###")
    # "name": "check_tax_adjustment_consistency
    last_query = state["messages"][-2]["content"]
    print(f"\nğŸ“© ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆ¤æ–­çµæœ:\n{last_query}")

    response_text = input("æ¬¡ã®ã‚„ã‚‹ã¹ãã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")

    # tools_ran ã‚’æ›´æ–°ï¼ˆè©²å½“ãƒ„ãƒ¼ãƒ«ã‚’å‰Šé™¤ï¼‰
    updated_tools_ran = set(state.get("tools_ran", set()))
    updated_tools_ran.discard("check_tax_adjustment_consistency")

    return {
        "messages": state["messages"] + [{
        "role": "user",
        "content": response_text,
        "id": str(uuid.uuid4())
        }],
        "tools_ran": updated_tools_ran
    }

# ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å‘¼ã³å‡ºã—ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
def custom_tool_node(state: dict) -> dict:
    # ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆLangGraphå†…éƒ¨ã‹ã‚‰ã®å‘¼ã³å‡ºã—ï¼‰
    tool_node = ChatAgentToolNode(tools)
    tool_result_state = tool_node.invoke(state, config=None)

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾—ï¼ˆAIMessage or dictï¼‰
    last_msg = state["messages"][-1]

    # tool_calls å–å¾—ï¼ˆä¸¡å¯¾å¿œï¼šdict or AIMessageï¼‰
    if isinstance(last_msg, dict):
        tool_calls = last_msg.get("tool_calls", [])
    else:
        tool_calls = getattr(last_msg, "tool_calls", [])

    # ã™ã§ã«å®Ÿè¡Œæ¸ˆã¿ã® tool ã®ã‚»ãƒƒãƒˆ
    tools_ran = set(state.get("tools_ran", set()))

    # å®Ÿè¡Œã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«åã‚’ tools_ran ã«è¿½åŠ 
    for call in tool_calls:
        # call è‡ªä½“ã‚‚ dict ã¾ãŸã¯ pydantic model ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ä¸¡å¯¾å¿œ
        if isinstance(call, dict):
            tool_name = call.get("name")
        else:
            tool_name = getattr(call, "name", None)
        if tool_name:
            tools_ran.add(tool_name)

    # æ›´æ–°å¾Œã®çŠ¶æ…‹ã« tools_ran ã‚’è¿½åŠ 
    tool_result_state["tools_ran"] = tools_ran
    return tool_result_state

#####################
## Define agent logic
#####################
# LangGraphãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰
def create_tool_calling_agent(
    model: LanguageModelLike,
    tools: Union[ToolNode, Sequence[BaseTool]],
    system_prompt: Optional[str] = None,
) -> CompiledGraph:
    # ãƒ¢ãƒ‡ãƒ«ã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰
    model = model.bind_tools(tools)

    # Define the function that determines which node to go to
    # æ¬¡ã®çŠ¶æ…‹ã‚’åˆ¤æ–­ã™ã‚‹é–¢æ•°
    def should_continue(state: MyState) -> str:
        last_msg = state["messages"][-1]
        tools_ran = state.get("tools_ran", set())
        if "tool_calls" in last_msg and last_msg["tool_calls"]:
            function_name = last_msg["tool_calls"][0]["function"]["name"]
            if function_name == "search_company_regulations":
                return "vector_search"
            return "tools"
        elif "check_tax_adjustment_consistency" in tools_ran:
            tools_ran.discard("check_tax_adjustment_consistency")
            return "human"
        elif "check_commuting_allowance_consistency" in tools_ran:
            tools_ran.discard("check_commuting_allowance_consistency")
            return "human"
        return "end"

    # å‰å‡¦ç†ï¼ˆå…¥åŠ›ã•ã‚ŒãŸ MyState ã‹ã‚‰ "messages" ã‚’å–ã‚Šå‡ºã—ã€å¿…è¦ãªã‚‰ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«è¿½åŠ ï¼‰
    if system_prompt:
        preprocessor = RunnableLambda(
            lambda state: [{"role": "system", "content": system_prompt}]
            + state["messages"]
        )
    else:
        preprocessor = RunnableLambda(lambda state: state["messages"])

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
    model_runnable = preprocessor | model
    
    def call_model(state: MyState, config: RunnableConfig):
        response = model_runnable.invoke(state, config)
        if response is None:
            return {"messages": []}

        return {"messages": [response]}

    # æ–°ã—ã„çŠ¶æ…‹stateã‚’å®šç¾©
    workflow = StateGraph(MyState)
    # nodeã‚’è¿½åŠ 
    workflow.add_node("agent", RunnableLambda(call_model))
    workflow.add_node("tools", RunnableLambda(custom_tool_node))
    workflow.add_node("human", RunnableLambda(human_assistance))
    workflow.add_node("vector_search", RunnableLambda(vector_search))

    ###########################
    ## çŠ¶æ…‹é·ç§»ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©
    ## agent â†’ "tool_calls"ã‚ã‚Š â†’ tools
    ## tools â†’ å‘¼ã³å‡ºã—å¾Œ â†’ agent ã«æˆ»ã‚‹ï¼ˆå†å¿œç­”ï¼‰
    ###########################
    # å…¥å£
    workflow.set_entry_point("agent")
    # æ¡ä»¶edgeè¿½åŠ 
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "human": "human",
            "vector_search": "vector_search",
            "end": END,
        },
    )
    # ä¸€èˆ¬edgeè¿½åŠ 
    workflow.add_edge("tools", "agent")
    workflow.add_edge("human", "agent")
    workflow.add_edge("vector_search", "tools")
    # ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ 
    memory = MemorySaver()
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    return workflow.compile(checkpointer=memory)

# å‡ºåŠ›çµæœã®å½¢ã‚’æ•´å½¢ã™ã‚‹
class LangGraphChatAgent(ChatAgent):
    def __init__(self, agent: CompiledStateGraph):
        self.agent = agent

    def predict(
        self,
        messages: list[ChatAgentMessage],
        context: Optional[ChatContext] = None,
        custom_inputs: Optional[dict[str, Any]] = None,
    ) -> ChatAgentResponse:
        request = {"messages": self._convert_messages_to_dict(messages)}
        config = {"configurable": {"thread_id": context.thread_id if context else "session_001"}, "callbacks": [tracer]}
        messages = []
        for event in self.agent.stream(request, config=config, stream_mode="updates"):
            for node_data in event.values():
                messages.extend(
                    ChatAgentMessage(**msg) for msg in node_data.get("messages", [])
                )
        return ChatAgentResponse(messages=messages)

    def predict_stream(
        self,
        messages: list[ChatAgentMessage],
        context: Optional[ChatContext] = None,
        custom_inputs: Optional[dict[str, Any]] = None,
    ) -> Generator[ChatAgentChunk, None, None]:
        request = {"messages": self._convert_messages_to_dict(messages)}
        config = {"configurable": {"thread_id": context.thread_id if context else "session_001"}, "callbacks": [tracer]}
        for event in self.agent.stream(request, config=config, stream_mode="updates"):
            for node_data in event.values():
                yield from (
                    ChatAgentChunk(**{"delta": msg}) for msg in node_data["messages"]
                )


# Create the agent object, and specify it as the agent object to use when
# loading the agent back for inference via mlflow.models.set_model()
# mlflow.langchain.autolog()
agent = create_tool_calling_agent(llm, tools, system_prompt)
AGENT = LangGraphChatAgent(agent)
# mlflow.models.set_model(AGENT)
